{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SpellChecker - Seq2Seq - TFv1.1 .ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPD8Y51iWBXFXM0sx3K6/lm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"TPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"nCe5i45hrP65","colab_type":"text"},"source":["Code:  https://github.com/Currie32/Spell-Checker/blob/master/SpellChecker.ipynb\n","\n","\n","Formato Texto: Hace referencias a los archivos de texto plano o texto enriquecido (.rtf por ejemplo)"]},{"cell_type":"code","metadata":{"id":"0dC_cB4OcJ3o","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":576},"executionInfo":{"status":"ok","timestamp":1583112704571,"user_tz":180,"elapsed":35725,"user":{"displayName":"Gabriel Marmanillo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhoQawd_HOZF-1I6-PnhN5cMx0-qOzohGrJQo47Gw=s64","userId":"07923766545187461618"}},"outputId":"adb24536-a6cc-4c87-82d6-8ead8a364868"},"source":["#!pip install tensorflow==1.1\n","#import tensorflow as tf\n","#print(tf.__version__)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting tensorflow==1.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/e4/b2a8bcd1fa689489050386ec70c5c547e4a75d06f2cc2b55f45463cd092c/tensorflow-1.1.0-cp36-cp36m-manylinux1_x86_64.whl (31.4MB)\n","\u001b[K     |████████████████████████████████| 31.4MB 141kB/s \n","\u001b[?25hRequirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.1) (1.0.0)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.1) (0.34.2)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.1) (1.12.0)\n","Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.1) (1.17.5)\n","Requirement already satisfied: protobuf>=3.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.1) (3.10.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.2.0->tensorflow==1.1) (45.1.0)\n","\u001b[31mERROR: stable-baselines 2.2.1 has requirement tensorflow>=1.5.0, but you'll have tensorflow 1.1.0 which is incompatible.\u001b[0m\n","\u001b[31mERROR: magenta 0.3.19 has requirement tensorflow>=1.12.0, but you'll have tensorflow 1.1.0 which is incompatible.\u001b[0m\n","Installing collected packages: tensorflow\n","  Found existing installation: tensorflow 1.15.0\n","    Uninstalling tensorflow-1.15.0:\n","      Successfully uninstalled tensorflow-1.15.0\n","Successfully installed tensorflow-1.1.0\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:455: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:456: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:457: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:458: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:459: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:462: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["1.1.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"og0JKkI06em6","colab_type":"text"},"source":["#***Importacion de Librerias***"]},{"cell_type":"code","metadata":{"id":"vYS-P1iWr34-","colab_type":"code","colab":{}},"source":["'''\n","* Importacion de Librerias necesarias para la ejecucion\n","'''\n","import pandas as pd\n","import numpy as np\n","import os\n","from os import listdir\n","from os.path import isfile, join\n","from collections import namedtuple\n","from tensorflow.python.layers.core import Dense\n","from tensorflow.python.ops.rnn_cell_impl import _zero_state_tensors\n","import time\n","import re\n","from sklearn.model_selection import train_test_split\n","from google.colab import drive"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6qfO8PWA6SEb","colab_type":"text"},"source":["#***Lectura del Corpus***"]},{"cell_type":"code","metadata":{"id":"CbDr-ChiwPcX","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1583103900436,"user_tz":180,"elapsed":702,"user":{"displayName":"Gabriel Marmanillo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhoQawd_HOZF-1I6-PnhN5cMx0-qOzohGrJQo47Gw=s64","userId":"07923766545187461618"}},"outputId":"c0e5e930-9440-4deb-f3a2-254da25bfe02"},"source":["'''\n","* Monta la unidad drive\n","'''\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LosDfIHpuWkn","colab_type":"code","colab":{}},"source":["'''\n","* Precondicion: Los libros deben estar en formato texto\n","* Input:  Directorio del libro\n","* Output: Texto del Libro\n","* Postcondicion: EL tipo de dato de book es String, lo cual debemos almacenar o utilizar ya que se reutiliza la variable\n","'''\n","def load_book(path):\n","    input_file = os.path.join(path)\n","    with open(input_file) as f:\n","        book = f.read()\n","    return book"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NQme7Hsdx0m5","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1583103896921,"user_tz":180,"elapsed":734,"user":{"displayName":"Gabriel Marmanillo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhoQawd_HOZF-1I6-PnhN5cMx0-qOzohGrJQo47Gw=s64","userId":"07923766545187461618"}},"outputId":"274e7683-0562-4720-fd27-1e821988ba11"},"source":["'''\n","* Precondicion: En el directorio debe exisitir y contener archivos en formato texto\n","* Input:  Directorio donde se encuentra los libros\n","* Output: Vector que contiene rutas de libros\n","* Postcondicion: Solo contiene la ruta para el uso constante de ello\n","Nota: Podemos reemplazar esta funcion con el ls usualmente utilizada\n","'''\n","path = '/content/gdrive/My Drive/DataSets/Corpus/'\n","book_files = [f for f in listdir(path) if isfile(join(path, f))]\n","print(book_files)\n","#book_files = book_files[1:]\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['Corpus.txt']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"toqbsdmq4Re_","colab_type":"text"},"source":["**NOTA** *Probar que ocurre si hay pdf, csv, entre otros formatos, de texto como se ve que toma?*"]},{"cell_type":"code","metadata":{"id":"COIIXOnvuWkt","colab_type":"code","colab":{}},"source":["'''\n","* Precondicion: Debe haber almenos un libro en formato texto\n","* Input:  Listado de directorio previamente obtenido\n","* Output: Vector de libros (String que contiene los libros)\n","* Postcondicion: Los textos son string, se puede realizar cualquier operacion que involucre de formato string\n","Nota: Si usamos ls, esta funcion nos permitira ahorrar el concatenamiento de la ruta y el directorio, siendo el codigo mas legible\n","'''\n","books = []\n","for book in book_files:\n","    books.append(load_book(path+book))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NIxoO4OCuWkx","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1583112743792,"user_tz":180,"elapsed":736,"user":{"displayName":"Gabriel Marmanillo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhoQawd_HOZF-1I6-PnhN5cMx0-qOzohGrJQo47Gw=s64","userId":"07923766545187461618"}},"outputId":"3afb42eb-20a8-4705-df55-da7c9b4ea955"},"source":["'''\n","* Precondicion: Debe haber ejecutado el proceso anterior y tener almenos un libro en el vector\n","* Input:  Vector de libros\n","* Output: Cantidad de palabras en ese libro\n","* Postcondicion: Ninguna\n","Nota: Este modulo nos ayudará a tener cierto control con la base de datos y la cantidad de palabras que estamos usando \n","'''\n","for i in range(len(books)):\n","    print(\"Hay {} cantidad de palabras en el libro {}.\".format(len(books[i].split()), book_files[i]))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Hay 16559 cantidad de palabras en el libro Corpus.txt.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1rBxnLmbuWk2","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1583112744908,"user_tz":180,"elapsed":447,"user":{"displayName":"Gabriel Marmanillo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhoQawd_HOZF-1I6-PnhN5cMx0-qOzohGrJQo47Gw=s64","userId":"07923766545187461618"}},"outputId":"760a754a-b538-40f0-c1ee-f619f24609e3"},"source":["'''\n","Vemos que obtenemos como salida de un cierto libro\n","'''\n","books[0][:500]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Me gusta el navegar en kayak.Los kiwis son una excelente fuente de vitamina C.\\n\\nPAPELES QUE A LA MUERTE DE GÜEMES\\nQUEDARON EN PODER DE LA FAMILIA\\n\\nLa primera información, apoyada en documentos, con que\\ncontamos sobre la suerte corrida por los \"papeles de Güemes\"\\nnos la dan las cartas, hoy en nuestro poder, que Martín, el hijo\\nprimogénito de Güemes, mientras acompañaba a algunos exilados\\ntíos maternos en el Perú, escribió en distintas oportunidades\\ndesde el Cerro de Pasco a su hermano Luis, de re'"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"7stUxGoR6D58","colab_type":"text"},"source":["# ***Preparacion del Corpus***"]},{"cell_type":"code","metadata":{"id":"HWlJT7jNuWk7","colab_type":"code","colab":{}},"source":["'''\n","* Precondicion: Un texto netamente puro\n","* Input:  El texto = El libro\n","* Output: Libro limpio sin caracteres especiales, tabulaciones, entre otros.\n","* Postcondicion: Ninguna\n","NOTA: Podemos usar el limpiador que utiliza el Dr. Xamena\n","'''\n","def clean_text(text):\n","    text = re.sub(r'\\n', ' ', text) \n","    text = re.sub(r'[{}@_*>()\\\\#%+=\\[\\]]','', text)\n","    text = re.sub('a0','', text)\n","    text = re.sub('\\'92t','\\'t', text)\n","    text = re.sub('\\'92s','\\'s', text)\n","    text = re.sub('\\'92m','\\'m', text)\n","    text = re.sub('\\'92ll','\\'ll', text)\n","    text = re.sub('\\'91','', text)\n","    text = re.sub('\\'92','', text)\n","    text = re.sub('\\'93','', text)\n","    text = re.sub('\\'94','', text)\n","    text = re.sub('\\.','. ', text)\n","    text = re.sub('\\!','! ', text)\n","    text = re.sub('\\?','? ', text)\n","    text = re.sub(' +',' ', text)\n","    return text"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-xj5b-ciuWk-","colab_type":"code","colab":{}},"source":["'''\n","* Precondicion: Array de libros sin limpiar\n","* Input: Array de libros\n","* Output: Array de libros sin caracteres especiales, etc., es decir, limpios.\n","'''\n","clean_books = []\n","for book in books:\n","    clean_books.append(clean_text(book))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"upxZLZM3uWlC","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1583112749488,"user_tz":180,"elapsed":518,"user":{"displayName":"Gabriel Marmanillo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhoQawd_HOZF-1I6-PnhN5cMx0-qOzohGrJQo47Gw=s64","userId":"07923766545187461618"}},"outputId":"d3176230-44d5-4a9a-c445-2c79cf2cf944"},"source":["'''\n","* Controlamos un texto alatorio como se ve para seguir refinando la limpieza\n","'''\n","clean_books[0][500:1500]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'reso éste en Salta desde el mismo lugar. Gracias a las del año 1847, sabemos lo siguiente: que, tras la muerte de Güemes 1821 y la de su viuda, doña Carmen Puch 1822, por minoridad de sus hijos, nacidos en 1817 y 1819 respectivamente, dichos papeles quedaron en manos de familiares cercanos; que quien después los tuvo en su poder y los ordenó fue el doctor José Redhead, el conocido médico de Güemes y de Belgrano, de gran prestigio y larga residencia en Salta “él fue el archivo de todo\", según tales cartas; que, muerto el nombrado facultativo el 3 de junio de 1846, Martín Güemes y Puch, en procura de que tan valiosos documentos prestaran pronta utilidad a la Historia, pidió una y otra vez a Luis que recogiera y le remitiera \"los papeles relativos a nuestro padre -decía- que debió dejar el doctor Redhead y algunas apuntaciones hechas por este sabio amigo\" , para, con don Manuel Puch, fío de ambos, hacer preparar una \"biografía\", y, concluida que fuese ésta, el último de los nombrados los '"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"MdziqbgYuWlH","colab_type":"code","colab":{}},"source":["'''\n","* Precondicion: Contar con un texto casi puro para codificar los caracteres en enteros para la aplicacion del modelo\n","* Input: Texto de libros limpios\n","* Output: Vocabulario codificado con entero\n","* Postcondicion: vocabulario(diccionario) codificado en entero, sin tokens especiales\n","'''\n","vocab_to_int = {}\n","count = 0\n","for book in clean_books:\n","    for character in book:\n","        if character not in vocab_to_int:\n","            vocab_to_int[character] = count\n","            count += 1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CViosj8j-4WA","colab_type":"code","colab":{}},"source":["'''\n","* Precondicion: vocabulario(diccionario) codificado con enteros\n","* Input: Vocabulario codificado con enteros sin caracteres especiales \n","* Output: Vocabulario codificado con enteros con caracteres especiales\n","* Postcondicion: Ninguna\n","Nota: Estos tokens especiales son especificamente para modelos seq2seq para el entrenamiento del mismo\n","'''\n","codes = ['<PAD>','<EOS>','<GO>','k']\n","for code in codes:\n","    vocab_to_int[code] = count\n","    count += 1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tVGImZq5uWlK","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":71},"executionInfo":{"status":"ok","timestamp":1583112754351,"user_tz":180,"elapsed":731,"user":{"displayName":"Gabriel Marmanillo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhoQawd_HOZF-1I6-PnhN5cMx0-qOzohGrJQo47Gw=s64","userId":"07923766545187461618"}},"outputId":"4e43fd4b-409b-487d-cb38-aadb7107e89e"},"source":["'''\n","Muestra el vocabulario ordenado\n","'''\n","print(\"El vocabulario contiene {} caracteres.\".format(len(vocab_to_int)))\n","print(sorted(vocab_to_int))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["El vocabulario contiene 101 caracteres.\n","['\\t', ' ', '!', '\"', '$', \"'\", ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '<EOS>', '<GO>', '<PAD>', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '¡', 'ª', '«', 'º', '»', '¿', 'Á', 'É', 'Í', 'Ñ', 'Ó', 'Ú', 'Ü', 'á', 'é', 'í', 'ñ', 'ó', 'ú', 'ü', '—', '“', '”']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ATJxC3tGuWlO","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":71},"executionInfo":{"status":"ok","timestamp":1583112755775,"user_tz":180,"elapsed":804,"user":{"displayName":"Gabriel Marmanillo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhoQawd_HOZF-1I6-PnhN5cMx0-qOzohGrJQo47Gw=s64","userId":"07923766545187461618"}},"outputId":"845bfb7a-0263-469c-fb26-0d35a6828c84"},"source":["'''\n","* Precondicion: Vocabulario(diccionario) codificado en enteros con caracteres especiales \n","* Input: Vocabulario(diccionario) codificado en enteros con caracteres especiales\n","* Output: Vocabulario(diccionario)\n","* Postcondicion: Ninguna\n","'''\n","int_to_vocab = {}\n","for character, value in vocab_to_int.items():\n","    int_to_vocab[value] = character\n","print(\"El vocabulario contiene {} caracteres.\".format(len(int_to_vocab)))\n","print(sorted(int_to_vocab))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["El vocabulario contiene 101 caracteres.\n","[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OsOYxfgCuWlQ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"executionInfo":{"status":"ok","timestamp":1583112757773,"user_tz":180,"elapsed":691,"user":{"displayName":"Gabriel Marmanillo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhoQawd_HOZF-1I6-PnhN5cMx0-qOzohGrJQo47Gw=s64","userId":"07923766545187461618"}},"outputId":"5c4fb593-f1f3-4cff-c949-fe1ac0b89658"},"source":["'''\n","* Precondicion: Libros limpios  \n","* Input: Array con libros limpios\n","* Output: Array de oraciones de los libros\n","* Postcondicion: Ninguna\n","Nota: A cada oracion del texto del libro lo convierte en oraciones separadas por la funcion split.\n","'''\n","sentences = []\n","for book in clean_books:\n","    for sentence in book.split('. '):\n","        sentences.append(sentence + '.')\n","print(\"Hay {} oraciones.\".format(len(sentences)))\n","\n","#Ejemplo\n","sentences[:2]"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Hay 916 oraciones.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["['Me gusta el navegar en kayak.',\n"," 'Los kiwis son una excelente fuente de vitamina C.']"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"code","metadata":{"id":"oZSrdxaKuWlV","colab_type":"code","colab":{}},"source":["'''\n","* Precondiciones: Array de oraciones\n","* Input: Array de oraciones\n","* Output: Array de oraciones, codificada los caracteres en enteros\n","* Postcondiciones: Ninguna\n","'''\n","int_sentences = []\n","\n","for sentence in sentences:\n","    int_sentence = []\n","    for character in sentence:\n","        int_sentence.append(vocab_to_int[character])\n","    int_sentences.append(int_sentence)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"I3uhiRzQuWlb","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":297},"executionInfo":{"status":"ok","timestamp":1583112761864,"user_tz":180,"elapsed":668,"user":{"displayName":"Gabriel Marmanillo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhoQawd_HOZF-1I6-PnhN5cMx0-qOzohGrJQo47Gw=s64","userId":"07923766545187461618"}},"outputId":"e58478b9-50e2-4331-9730-008fb14f1924"},"source":["'''\n","* Precondiciones: oraciones codificada en enteros\n","* Input: Array de oraciones codificada en enteros\n","* Output: DataFrame de las oraciones\n","* Postcondiciones: Dataframe hecho en pandas para poder sacar facilmente estadisticos\n","'''\n","lengths = []\n","for sentence in int_sentences:\n","    lengths.append(len(sentence))\n","lengths = pd.DataFrame(lengths, columns=[\"counts\"])\n","lengths.describe()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>counts</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>916.000000</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>104.156114</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>130.711758</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>1.000000</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>4.000000</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>61.000000</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>149.000000</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>1189.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["            counts\n","count   916.000000\n","mean    104.156114\n","std     130.711758\n","min       1.000000\n","25%       4.000000\n","50%      61.000000\n","75%     149.000000\n","max    1189.000000"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"code","metadata":{"id":"taUQESkzuWlh","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1583112765070,"user_tz":180,"elapsed":684,"user":{"displayName":"Gabriel Marmanillo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhoQawd_HOZF-1I6-PnhN5cMx0-qOzohGrJQo47Gw=s64","userId":"07923766545187461618"}},"outputId":"97c80ed7-d7f5-4818-eaec-a338b7fe41b4"},"source":["'''\n","* Precondicion: array de oraciones codificadas en entero \n","* Input: array de oraciones codificadas en entero\n","* Output: cantidad de oraciones que se usaran para el entrenamiento y la prueba\n","* Postcondiciones: Ninguna\n","Nota: Filtraremos oraciones cortas o muy larga para el modelo este es la ventana que se puede modificar.\n","'''\n","max_length = 92\n","min_length = 10\n","\n","good_sentences = []\n","\n","for sentence in int_sentences:\n","    if len(sentence) <= max_length and len(sentence) >= min_length:\n","        good_sentences.append(sentence)\n","\n","print(\"Usaremos {} para entrenar y probar nuestro modelo.\".format(len(good_sentences)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Usaremos 293 para entrenar y probar nuestro modelo.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LBzlifUauWll","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1583112767095,"user_tz":180,"elapsed":677,"user":{"displayName":"Gabriel Marmanillo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhoQawd_HOZF-1I6-PnhN5cMx0-qOzohGrJQo47Gw=s64","userId":"07923766545187461618"}},"outputId":"40cc7973-501e-44de-8dd9-f4297b15c594"},"source":["'''\n","Tomaremos solo el 0.15% de las oraciones para probar y el resto sera para entrenamiento\n","'''\n","training, testing = train_test_split(good_sentences, test_size = 0.15, random_state = 2)\n","\n","print(\"Oraciones para Entrenamiento:\", len(training))\n","print(\"Oraciones para prueba:\", len(testing))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Oraciones para Entrenamiento: 249\n","Oraciones para prueba: 44\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"NCSUFOy5uWlo","colab_type":"code","colab":{}},"source":["'''\n","Ordenamos las oraciones para que el entrenamiento sea mas rapido y la ventana vaya creciendo de a poco\n","'''\n","training_sorted = []\n","testing_sorted = []\n","\n","for i in range(min_length, max_length+1):\n","    for sentence in training:\n","        if len(sentence) == i:\n","            training_sorted.append(sentence)\n","    for sentence in testing:\n","        if len(sentence) == i:\n","            testing_sorted.append(sentence)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xAv9-LJXuWlq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":102},"executionInfo":{"status":"ok","timestamp":1583112770628,"user_tz":180,"elapsed":684,"user":{"displayName":"Gabriel Marmanillo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhoQawd_HOZF-1I6-PnhN5cMx0-qOzohGrJQo47Gw=s64","userId":"07923766545187461618"}},"outputId":"cd7c0a41-119a-4ff1-f48f-668426ea9c5d"},"source":["'''\n","Veremos que las oraciones esten ordenando correctamente\n","'''\n","for i in range(5):\n","    print(\"Oracion:  \" + str( training_sorted[i] ) + \" Longitud: \" + str( len(training_sorted[i]) ) )"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Oracion:  [1, 8, 2, 43, 4, 1, 2, 30, 22, 14] Longitud: 10\n","Oracion:  [42, 2, 33, 1, 5, 6, 17, 9, 16, 5, 14] Longitud: 11\n","Oracion:  [42, 2, 1, 9, 2, 25, 16, 6, 16, 5, 48, 14] Longitud: 12\n","Oracion:  [23, 4, 20, 47, 16, 5, 2, 7, 51, 16, 5, 14] Longitud: 12\n","Oracion:  [23, 4, 20, 47, 16, 5, 2, 7, 51, 16, 5, 14] Longitud: 12\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tlO-kjo-uWlt","colab_type":"code","colab":{}},"source":["'''\n","* Precondicion: oraciones y una probabilidad menor a 1 previamentes definidas\n","* Input: Array de oraciones codificadas y una probabilidad\n","* Output: Arrya de oraciones mal escritas\n","* Postcondiciones:  Ninguna\n","Crea errores en las oraciones, si la probabilidad es menor a threshold:\n","* si es mayor al 67% quita un caracter\n","  sino reordena un caracter\n","* con el 33% restante agrega un caracter \n","'''\n","\n","letters = ['a','b','c','d','e','f','g','h','i','j','k','l','m',\n","           'n','o','p','q','r','s','t','u','v','w','x','y','z',]\n","\n","def noise_maker(sentence, threshold):\n","    noisy_sentence = []\n","    i = 0\n","    while i < len(sentence):\n","        random = np.random.uniform(0,1,1)\n","        if random < threshold:\n","            noisy_sentence.append(sentence[i])\n","        else:\n","            new_random = np.random.uniform(0,1,1)\n","            if new_random > 0.67:\n","                if i == (len(sentence) - 1):\n","                    continue\n","                else:\n","                    noisy_sentence.append(sentence[i+1])\n","                    noisy_sentence.append(sentence[i])\n","                    i += 1\n","            elif new_random < 0.33:\n","                random_letter = np.random.choice(letters, 1)[0]\n","                noisy_sentence.append(vocab_to_int[random_letter])\n","                noisy_sentence.append(sentence[i])\n","            else:\n","                pass     \n","        i += 1\n","    return noisy_sentence"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"keT2UHGIuWlw","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":272},"executionInfo":{"status":"ok","timestamp":1583112775270,"user_tz":180,"elapsed":719,"user":{"displayName":"Gabriel Marmanillo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhoQawd_HOZF-1I6-PnhN5cMx0-qOzohGrJQo47Gw=s64","userId":"07923766545187461618"}},"outputId":"56b5c494-c92f-4909-b8f2-f7dc5d6fca85"},"source":["'''\n","Muestras algunas oraciones creadas\n","'''\n","threshold = 0.9\n","for sentence in training_sorted[:5]:\n","    print(sentence)\n","    print(noise_maker(sentence, threshold))\n","    print()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[1, 8, 2, 43, 4, 1, 2, 30, 22, 14]\n","[1, 8, 2, 43, 4, 1, 2, 30, 22, 14]\n","\n","[42, 2, 33, 1, 5, 6, 17, 9, 16, 5, 14]\n","[42, 2, 33, 1, 5, 6, 17, 9, 16, 5, 14]\n","\n","[42, 2, 1, 9, 2, 25, 16, 6, 16, 5, 48, 14]\n","[42, 2, 9, 25, 2, 1, 16, 6, 16, 5, 48, 14]\n","\n","[23, 4, 20, 47, 16, 5, 2, 7, 51, 16, 5, 14]\n","[23, 4, 20, 47, 9, 16, 5, 2, 7, 51, 16, 5, 14]\n","\n","[23, 4, 20, 47, 16, 5, 2, 7, 51, 16, 5, 14]\n","[23, 4, 20, 47, 16, 5, 20, 2, 51, 16]\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"pIt_OAvpWCwK","colab_type":"text"},"source":["# **Construccion del Modelo**"]},{"cell_type":"code","metadata":{"id":"V96voLBxWIJz","colab_type":"code","colab":{}},"source":["'''\n","Crea los marcadores de entrada del modelo\n","Nota en tensorFlow 2.x ya estos marcadores estan obsolotes y no se usan\n","'''\n","\n","def model_inputs():    \n","    with tf.name_scope('inputs'): #\n","        inputs = tf.placeholder(tf.int32, [None, None], name='inputs')\n","    with tf.name_scope('targets'):\n","        targets = tf.placeholder(tf.int32, [None, None], name='targets')\n","    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n","    inputs_length = tf.placeholder(tf.int32, (None,), name='inputs_length')\n","    targets_length = tf.placeholder(tf.int32, (None,), name='targets_length')\n","    max_target_length = tf.reduce_max(targets_length, name='max_target_len')\n","\n","    return inputs, targets, keep_prob, inputs_length, targets_length, max_target_length"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HU9afX8_uWl6","colab_type":"code","colab":{}},"source":["'''\n","* Precondiciones:\n","* Input:\n","* Output:\n","* Postcondiciones:\n","'''\n","def process_encoding_input(targets, vocab_to_int, batch_size):\n","    with tf.name_scope(\"process_encoding\"):\n","        ending = tf.strided_slice(targets, [0, 0], [batch_size, -1], [1, 1])\n","        dec_input = tf.concat([tf.fill([batch_size, 1], vocab_to_int['<GO>']), ending], 1)\n","\n","    return dec_input"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7oK5ysL5uWl-","colab_type":"code","colab":{}},"source":["'''\n","* Precondiciones:\n","* Input:\n","* Output:\n","* Postcondiciones:\n","'''\n","def encoding_layer(rnn_size, sequence_length, num_layers, rnn_inputs, keep_prob, direction): \n","    if direction == 1:\n","        with tf.name_scope(\"RNN_Encoder_Cell_1D\"):\n","            for layer in range(num_layers):\n","                with tf.variable_scope('encoder_{}'.format(layer)):\n","                    lstm = tf.contrib.rnn.LSTMCell(rnn_size)\n","\n","                    drop = tf.contrib.rnn.DropoutWrapper(lstm, \n","                                                         input_keep_prob = keep_prob)\n","\n","                    enc_output, enc_state = tf.nn.dynamic_rnn(drop, \n","                                                              rnn_inputs,\n","                                                              sequence_length,\n","                                                              dtype=tf.float32)\n","\n","            return enc_output, enc_state\n","          \n","    if direction == 2:\n","        with tf.name_scope(\"RNN_Encoder_Cell_2D\"):\n","            for layer in range(num_layers):\n","                with tf.variable_scope('encoder_{}'.format(layer)):\n","                    cell_fw = tf.contrib.rnn.LSTMCell(rnn_size)\n","                    cell_fw = tf.contrib.rnn.DropoutWrapper(cell_fw, \n","                                                            input_keep_prob = keep_prob)\n","\n","                    cell_bw = tf.contrib.rnn.LSTMCell(rnn_size)\n","                    cell_bw = tf.contrib.rnn.DropoutWrapper(cell_bw, \n","                                                            input_keep_prob = keep_prob)\n","\n","                    enc_output, enc_state = tf.nn.bidirectional_dynamic_rnn(cell_fw, \n","                                                                            cell_bw, \n","                                                                            rnn_inputs,\n","                                                                            sequence_length,\n","                                                                            dtype=tf.float32)\n","            enc_output = tf.concat(enc_output,2)\n","            return enc_output, enc_state[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gezwezw-uWmD","colab_type":"code","colab":{}},"source":["'''\n","* Precondiciones:\n","* Input:\n","* Output:\n","* Postcondiciones:\n","'''\n","def training_decoding_layer(dec_embed_input, targets_length, dec_cell, initial_state, output_layer, \n","                            vocab_size, max_target_length):\n","    with tf.name_scope(\"Training_Decoder\"):\n","        training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_embed_input,\n","                                                            sequence_length=targets_length,\n","                                                            time_major=False)\n","\n","        training_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n","                                                           training_helper,\n","                                                           initial_state,\n","                                                           output_layer) \n","\n","        training_logits, _ = tf.contrib.seq2seq.dynamic_decode(training_decoder,\n","                                                               output_time_major=False,\n","                                                               impute_finished=True,\n","                                                               maximum_iterations=max_target_length)\n","        return training_logits"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vf5784iLuWmG","colab_type":"code","colab":{}},"source":["'''\n","* Precondiciones:\n","* Input:\n","* Output:\n","* Postcondiciones:\n","'''\n","def inference_decoding_layer(embeddings, start_token, end_token, dec_cell, initial_state, output_layer,\n","                             max_target_length, batch_size):\n","    with tf.name_scope(\"Inference_Decoder\"):\n","        start_tokens = tf.tile(tf.constant([start_token], dtype=tf.int32), [batch_size], name='start_tokens')\n","\n","        inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embeddings,\n","                                                                    start_tokens,\n","                                                                    end_token)\n","\n","        inference_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n","                                                            inference_helper,\n","                                                            initial_state,\n","                                                            output_layer)\n","\n","        inference_logits, _ = tf.contrib.seq2seq.dynamic_decode(inference_decoder,\n","                                                                output_time_major=False,\n","                                                                impute_finished=True,\n","                                                                maximum_iterations=max_target_length)\n","\n","        return inference_logits"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MCJUk0NGuWmJ","colab_type":"code","colab":{}},"source":["'''\n","* Precondiciones:\n","* Input:\n","* Output:\n","* Postcondiciones:\n","'''\n","def decoding_layer(dec_embed_input, embeddings, enc_output, enc_state, vocab_size, inputs_length, targets_length, \n","                   max_target_length, rnn_size, vocab_to_int, keep_prob, batch_size, num_layers, direction):   \n","    with tf.name_scope(\"RNN_Decoder_Cell\"):\n","        for layer in range(num_layers):\n","            with tf.variable_scope('decoder_{}'.format(layer)):\n","                lstm = tf.contrib.rnn.LSTMCell(rnn_size)\n","                dec_cell = tf.contrib.rnn.DropoutWrapper(lstm, \n","                                                         input_keep_prob = keep_prob)\n","    \n","    output_layer = Dense(vocab_size,\n","                         kernel_initializer = tf.truncated_normal_initializer(mean = 0.0, stddev=0.1))\n","    \n","    attn_mech = tf.contrib.seq2seq.BahdanauAttention(rnn_size,\n","                                                  enc_output,\n","                                                  inputs_length,\n","                                                  normalize=False,\n","                                                  name='BahdanauAttention')\n","    \n","    with tf.name_scope(\"Attention_Wrapper\"):\n","        dec_cell = tf.contrib.seq2seq.DynamicAttentionWrapper(dec_cell,\n","                                                              attn_mech,\n","                                                              rnn_size)\n","    \n","    initial_state = tf.contrib.seq2seq.DynamicAttentionWrapperState(enc_state,\n","                                                                    _zero_state_tensors(rnn_size, \n","                                                                                        batch_size, \n","                                                                                        tf.float32))\n","\n","    with tf.variable_scope(\"decode\"):\n","        training_logits = training_decoding_layer(dec_embed_input, \n","                                                  targets_length, \n","                                                  dec_cell, \n","                                                  initial_state,\n","                                                  output_layer,\n","                                                  vocab_size, \n","                                                  max_target_length)\n","    with tf.variable_scope(\"decode\", reuse=True):\n","        inference_logits = inference_decoding_layer(embeddings,  \n","                                                    vocab_to_int['<GO>'], \n","                                                    vocab_to_int['<EOS>'],\n","                                                    dec_cell, \n","                                                    initial_state, \n","                                                    output_layer,\n","                                                    max_target_length,\n","                                                    batch_size)\n","\n","    return training_logits, inference_logits"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WgRVj-TxuWmO","colab_type":"code","colab":{}},"source":["'''\n","* Precondiciones:\n","* Input:\n","* Output:\n","* Postcondiciones:\n","'''\n","def seq2seq_model(inputs, targets, keep_prob, inputs_length, targets_length, max_target_length, \n","                  vocab_size, rnn_size, num_layers, vocab_to_int, batch_size, embedding_size, direction):\n","   \n","    enc_embeddings = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1, 1))\n","    enc_embed_input = tf.nn.embedding_lookup(enc_embeddings, inputs)\n","    enc_output, enc_state = encoding_layer(rnn_size, inputs_length, num_layers, \n","                                           enc_embed_input, keep_prob, direction)\n","    \n","    dec_embeddings = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1, 1))\n","    dec_input = process_encoding_input(targets, vocab_to_int, batch_size)\n","    dec_embed_input = tf.nn.embedding_lookup(dec_embeddings, dec_input)\n","    \n","    training_logits, inference_logits  = decoding_layer(dec_embed_input, \n","                                                        dec_embeddings,\n","                                                        enc_output,\n","                                                        enc_state, \n","                                                        vocab_size, \n","                                                        inputs_length, \n","                                                        targets_length, \n","                                                        max_target_length,\n","                                                        rnn_size, \n","                                                        vocab_to_int, \n","                                                        keep_prob, \n","                                                        batch_size,\n","                                                        num_layers,\n","                                                        direction)\n","    \n","    return training_logits, inference_logits"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7sGo_f2duWmQ","colab_type":"code","colab":{}},"source":["'''\n","* Precondiciones:\n","* Input:\n","* Output:\n","* Postcondiciones:\n","'''\n","def pad_sentence_batch(sentence_batch):\n","    max_sentence = max([len(sentence) for sentence in sentence_batch])\n","    return [sentence + [vocab_to_int['<PAD>']] * (max_sentence - len(sentence)) for sentence in sentence_batch]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ptphl44LuWmU","colab_type":"code","colab":{}},"source":["'''\n","* Precondiciones:\n","* Input:\n","* Output:\n","* Postcondiciones:\n","'''\n","def get_batches(sentences, batch_size, threshold):\n","\n","    for batch_i in range(0, len(sentences)//batch_size):\n","        start_i = batch_i * batch_size\n","        sentences_batch = sentences[start_i:start_i + batch_size]\n","        \n","        sentences_batch_noisy = []\n","        for sentence in sentences_batch:\n","            sentences_batch_noisy.append(noise_maker(sentence, threshold))\n","            \n","        sentences_batch_eos = []\n","        for sentence in sentences_batch:\n","            sentence.append(vocab_to_int['<EOS>'])\n","            sentences_batch_eos.append(sentence)\n","            \n","        pad_sentences_batch = np.array(pad_sentence_batch(sentences_batch_eos))\n","        pad_sentences_noisy_batch = np.array(pad_sentence_batch(sentences_batch_noisy))\n","        \n","        pad_sentences_lengths = []\n","        for sentence in pad_sentences_batch:\n","            pad_sentences_lengths.append(len(sentence))\n","        \n","        pad_sentences_noisy_lengths = []\n","        for sentence in pad_sentences_noisy_batch:\n","            pad_sentences_noisy_lengths.append(len(sentence))\n","        \n","        yield pad_sentences_noisy_batch, pad_sentences_batch, pad_sentences_noisy_lengths, pad_sentences_lengths"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ka_labNpuWmY","colab_type":"code","colab":{}},"source":["# Parametros por default\n","epochs = 100\n","batch_size = 128\n","num_layers = 2\n","rnn_size = 512\n","embedding_size = 128\n","learning_rate = 0.0005\n","direction = 2\n","threshold = 0.95\n","keep_probability = 0.75"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ydn1VBvAuWmc","colab_type":"code","colab":{}},"source":["'''\n","* Precondiciones:\n","* Input:\n","* Output:\n","* Postcondiciones:\n","'''\n","def build_graph(keep_prob, rnn_size, num_layers, batch_size, learning_rate, embedding_size, direction):\n","\n","    tf.reset_default_graph()\n","\n","    inputs, targets, keep_prob, inputs_length, targets_length, max_target_length = model_inputs()\n","\n","    training_logits, inference_logits = seq2seq_model(tf.reverse(inputs, [-1]),\n","                                                      targets, \n","                                                      keep_prob,   \n","                                                      inputs_length,\n","                                                      targets_length,\n","                                                      max_target_length,\n","                                                      len(vocab_to_int)+1,\n","                                                      rnn_size, \n","                                                      num_layers, \n","                                                      vocab_to_int,\n","                                                      batch_size,\n","                                                      embedding_size,\n","                                                      direction)\n","\n","    training_logits = tf.identity(training_logits.rnn_output, 'logits')\n","\n","    with tf.name_scope('predictions'):\n","        predictions = tf.identity(inference_logits.sample_id, name='predictions')\n","        tf.summary.histogram('predictions', predictions)\n","    masks = tf.sequence_mask(targets_length, max_target_length, dtype=tf.float32, name='masks')\n","    \n","    with tf.name_scope(\"cost\"):\n","        cost = tf.contrib.seq2seq.sequence_loss(training_logits, \n","                                                targets, \n","                                                masks)\n","        tf.summary.scalar('cost', cost)\n","\n","    with tf.name_scope(\"optimze\"):\n","        optimizer = tf.train.AdamOptimizer(learning_rate)\n","\n","        gradients = optimizer.compute_gradients(cost)\n","        capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\n","        train_op = optimizer.apply_gradients(capped_gradients)\n","\n","    merged = tf.summary.merge_all()    \n","\n","    export_nodes = ['inputs', 'targets', 'keep_prob', 'cost', 'inputs_length', 'targets_length',\n","                    'predictions', 'merged', 'train_op','optimizer']\n","    Graph = namedtuple('Graph', export_nodes)\n","    local_dict = locals()\n","    graph = Graph(*[local_dict[each] for each in export_nodes])\n","\n","    return graph"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_bS7oUsMW-FV","colab_type":"text"},"source":["# **Entrenamiento del Modelo**"]},{"cell_type":"code","metadata":{"scrolled":false,"id":"BfYIQmM0uWmf","colab_type":"code","colab":{}},"source":["'''\n","* Precondiciones:\n","* Input:\n","* Output:\n","* Postcondiciones:\n","'''\n","def train(model, epochs, log_string):\n","\n","    with tf.Session() as sess:\n","        sess.run(tf.global_variables_initializer())\n","\n","        testing_loss_summary = []\n","\n","        iteration = 0\n","        \n","        display_step = 30 \n","        stop_early = 0 \n","        stop = 3 \n","        per_epoch = 3 \n","        testing_check = (len(training_sorted)//batch_size//per_epoch)-1\n","\n","        print()\n","        print(\"Training Model: {}\".format(log_string))\n","\n","        train_writer = tf.summary.FileWriter('./logs/1/train/{}'.format(log_string), sess.graph)\n","        test_writer = tf.summary.FileWriter('./logs/1/test/{}'.format(log_string))\n","      \n","        for epoch_i in range(1, epochs+1): \n","            batch_loss = 0\n","            batch_time = 0\n","            \n","            for batch_i, (input_batch, target_batch, input_length, target_length) in enumerate(\n","                    get_batches(training_sorted, batch_size, threshold)):\n","                start_time = time.time()\n","                print(start_time)\n","                summary, loss, _ = sess.run([model.merged,\n","                                             model.cost, \n","                                             model.train_op], \n","                                             {model.inputs: input_batch,\n","                                              model.targets: target_batch,\n","                                              model.inputs_length: input_length,\n","                                              model.targets_length: target_length,\n","                                              model.keep_prob: keep_probability})\n","\n","                print(summary)\n","                batch_loss += loss\n","                end_time = time.time()\n","                batch_time += end_time - start_time\n","                train_writer.add_summary(summary, iteration)\n","\n","                iteration += 1\n","                print(iteration)\n","                if batch_i % display_step == 0 and batch_i > 0:\n","                    print('Epoch {:>3}/{} Batch {:>4}/{} - Loss: {:>6.3f}, Seconds: {:>4.2f}'\n","                          .format(epoch_i,\n","                                  epochs, \n","                                  batch_i, \n","                                  len(training_sorted) // batch_size, \n","                                  batch_loss / display_step, \n","                                  batch_time))\n","                    batch_loss = 0\n","                    batch_time = 0\n","\n","                if batch_i % testing_check == 0 and batch_i > 0:\n","                    batch_loss_testing = 0\n","                    batch_time_testing = 0\n","                    for batch_i, (input_batch, target_batch, input_length, target_length) in enumerate(\n","                            get_batches(testing_sorted, batch_size, threshold)):\n","                        start_time_testing = time.time()\n","                        summary, loss = sess.run([model.merged,\n","                                                  model.cost], \n","                                                     {model.inputs: input_batch,\n","                                                      model.targets: target_batch,\n","                                                      model.inputs_length: input_length,\n","                                                      model.targets_length: target_length,\n","                                                      model.keep_prob: 1})\n","\n","                        batch_loss_testing += loss\n","                        end_time_testing = time.time()\n","                        batch_time_testing += end_time_testing - start_time_testing\n","\n","                        test_writer.add_summary(summary, iteration)\n","\n","                    n_batches_testing = batch_i + 1\n","                    print('Testing Loss: {:>6.3f}, Seconds: {:>4.2f}'\n","                          .format(batch_loss_testing / n_batches_testing, \n","                                  batch_time_testing))\n","                    \n","                    batch_time_testing = 0\n","\n","                    testing_loss_summary.append(batch_loss_testing)\n","                    if batch_loss_testing <= min(testing_loss_summary):\n","                        print('New Record!') \n","                        stop_early = 0\n","                        checkpoint = \"./{}.ckpt\".format(log_string)\n","                        saver = tf.train.Saver()\n","                        saver.save(sess, checkpoint)\n","\n","                    else:\n","                        print(\"No Improvement.\")\n","                        stop_early += 1\n","                        if stop_early == stop:\n","                            break\n","\n","            if stop_early == stop:\n","                print(\"Stopping Training.\")\n","                break\n","                "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"VXBq1jNYuWmh","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":425},"executionInfo":{"status":"error","timestamp":1583113168450,"user_tz":180,"elapsed":50304,"user":{"displayName":"Gabriel Marmanillo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhoQawd_HOZF-1I6-PnhN5cMx0-qOzohGrJQo47Gw=s64","userId":"07923766545187461618"}},"outputId":"7ec6bebd-5d49-49b2-e4d2-ea3179fd7e15"},"source":["'''\n","* Precondiciones:\n","* Input:\n","* Output:\n","* Postcondiciones:\n","'''\n","for keep_probability in [0.75]:\n","    for num_layers in [2]:\n","        for threshold in [0.95]:\n","            log_string = 'kp={},nl={},th={}'.format(keep_probability,\n","                                                    num_layers,\n","                                                    threshold) \n","            print(\"true\")\n","            model = build_graph(keep_probability, rnn_size, num_layers, batch_size, \n","                                learning_rate, embedding_size, direction)\n","            print(\"true\")\n","            train(model, epochs, log_string)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["true\n","true\n","\n","Training Model: kp=0.75,nl=2,th=0.95\n","1583102321.0712469\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-43-fbbaaf131b5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m                                 learning_rate, embedding_size, direction)\n\u001b[1;32m     16\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"true\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-42-abf6e5264325>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, epochs, log_string)\u001b[0m\n\u001b[1;32m     41\u001b[0m                                               \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minput_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m                                               \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargets_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtarget_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m                                               model.keep_prob: keep_probability})\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"rm8AK4rFWu42","colab_type":"text"},"source":["# **Oraciones Erroneas Creadas**"]},{"cell_type":"code","metadata":{"id":"PK2zgS6uuWmn","colab_type":"code","colab":{}},"source":["'''\n","* Precondiciones:\n","* Input:\n","* Output:\n","* Postcondiciones:\n","'''\n","def text_to_ints(text):\n","   \n","    text = clean_text(text)\n","    return [vocab_to_int[word] for word in text]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"X-Fq7dv4uWmq","colab_type":"code","colab":{}},"source":["\n","text = \"Spellin is difficult, whch is wyh you need to study everyday.\"\n","text = text_to_ints(text)\n","\n","\n","checkpoint = \"./kp=0.75,nl=2,th=0.95.ckpt\"\n","\n","model = build_graph(keep_probability, rnn_size, num_layers, batch_size, learning_rate, embedding_size, direction) \n","\n","with tf.Session() as sess:\n","    saver = tf.train.Saver()\n","    saver.restore(sess, checkpoint)\n","    \n","    answer_logits = sess.run(model.predictions, {model.inputs: [text]*batch_size, \n","                                                 model.inputs_length: [len(text)]*batch_size,\n","                                                 model.targets_length: [len(text)+1], \n","                                                 model.keep_prob: [1.0]})[0]\n","\n","pad = vocab_to_int[\"<PAD>\"] \n","\n","print('\\nText')\n","print('  Word Ids:    {}'.format([i for i in text]))\n","print('  Input Words: {}'.format(\"\".join([int_to_vocab[i] for i in text])))\n","\n","print('\\nSummary')\n","print('  Word Ids:       {}'.format([i for i in answer_logits if i != pad]))\n","print('  Response Words: {}'.format(\"\".join([int_to_vocab[i] for i in answer_logits if i != pad])))"],"execution_count":null,"outputs":[]}]}