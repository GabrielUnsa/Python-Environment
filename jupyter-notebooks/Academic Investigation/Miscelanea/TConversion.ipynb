{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TConversion.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNxQmI8cC7solt1shiwUY0a"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"LY6rhaXQ5HZ_","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596407200390,"user_tz":180,"elapsed":778,"user":{"displayName":"Gabriel Marmanillo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj5grV-7OJ46EaNgmQcb0uLo0WmZiuW09h1hNV7-A=s64","userId":"07923766545187461618"}}},"source":["import time\n","import numpy as np\n","import tensorflow as tf"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"gXXHiiIDvOTB","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":102},"executionInfo":{"status":"ok","timestamp":1596407506464,"user_tz":180,"elapsed":7683,"user":{"displayName":"Gabriel Marmanillo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj5grV-7OJ46EaNgmQcb0uLo0WmZiuW09h1hNV7-A=s64","userId":"07923766545187461618"}},"outputId":"c4712a65-64fa-46c6-f872-3a8b47213b24"},"source":["get_ipython().system('mkdir data')\n","get_ipython().system('wget -q -O data/ptb.zip https://ibm.box.com/shared/static/z2yvmhbskc45xd2a9a4kkn6hg4g4kj5r.zip')\n","get_ipython().system('unzip -o data/ptb.zip -d data')\n","get_ipython().system('cp data/ptb/reader.py .')\n","\n","import reader"],"execution_count":14,"outputs":[{"output_type":"stream","text":["mkdir: cannot create directory ‘data’: File exists\n","Archive:  data/ptb.zip\n","  inflating: data/ptb/reader.py      \n","  inflating: data/__MACOSX/ptb/._reader.py  \n","  inflating: data/__MACOSX/._ptb     \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"g6HdRiTMvqqA","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":224},"executionInfo":{"status":"ok","timestamp":1596407226975,"user_tz":180,"elapsed":16639,"user":{"displayName":"Gabriel Marmanillo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj5grV-7OJ46EaNgmQcb0uLo0WmZiuW09h1hNV7-A=s64","userId":"07923766545187461618"}},"outputId":"216e77e1-f575-42bf-8719-cc5caec3be7f"},"source":["get_ipython().system('wget http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz ')\n","get_ipython().system('tar xzf simple-examples.tgz -C data/')"],"execution_count":9,"outputs":[{"output_type":"stream","text":["--2020-08-02 22:26:51--  http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz\n","Resolving www.fit.vutbr.cz (www.fit.vutbr.cz)... 147.229.9.23, 2001:67c:1220:809::93e5:917\n","Connecting to www.fit.vutbr.cz (www.fit.vutbr.cz)|147.229.9.23|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 34869662 (33M) [application/x-gtar]\n","Saving to: ‘simple-examples.tgz.1’\n","\n","simple-examples.tgz 100%[===================>]  33.25M  3.00MB/s    in 12s     \n","\n","2020-08-02 22:27:03 (2.70 MB/s) - ‘simple-examples.tgz.1’ saved [34869662/34869662]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"T8APIYxfvwcO","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596407226978,"user_tz":180,"elapsed":14013,"user":{"displayName":"Gabriel Marmanillo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj5grV-7OJ46EaNgmQcb0uLo0WmZiuW09h1hNV7-A=s64","userId":"07923766545187461618"}}},"source":["#Initial weight scale\n","init_scale = 0.1\n","#Initial learning rate\n","learning_rate = 1.0\n","#Maximum permissible norm for the gradient (For gradient clipping -- another measure against Exploding Gradients)\n","max_grad_norm = 5\n","#The number of layers in our model\n","num_layers = 2\n","#The total number of recurrence steps, also known as the number of layers when our RNN is \"unfolded\"\n","num_steps = 20\n","#The number of processing units (neurons) in the hidden layers\n","hidden_size_l1 = 256\n","hidden_size_l2 = 128\n","#The maximum number of epochs trained with the initial learning rate\n","max_epoch_decay_lr = 4\n","#The total number of epochs in training\n","max_epoch = 15\n","#The probability for keeping data in the Dropout Layer (This is an optimization, but is outside our scope for this notebook!)\n","#At 1, we ignore the Dropout Layer wrapping.\n","keep_prob = 1\n","#The decay for the learning rate\n","decay = 0.5\n","#The size for each batch of data\n","batch_size = 60\n","#The size of our vocabulary\n","vocab_size = 10000\n","embeding_vector_size = 200\n","#Training flag to separate training from testing\n","is_training = 1\n","#Data directory for our dataset\n","data_dir = \"data/simple-examples/data/\""],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"-y4SkQyXv3c_","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596407236344,"user_tz":180,"elapsed":9350,"user":{"displayName":"Gabriel Marmanillo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj5grV-7OJ46EaNgmQcb0uLo0WmZiuW09h1hNV7-A=s64","userId":"07923766545187461618"}}},"source":["session = tf.compat.v1.InteractiveSession()"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"Db0hTSZgv653","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":323},"executionInfo":{"status":"error","timestamp":1596409849453,"user_tz":180,"elapsed":799,"user":{"displayName":"Gabriel Marmanillo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj5grV-7OJ46EaNgmQcb0uLo0WmZiuW09h1hNV7-A=s64","userId":"07923766545187461618"}},"outputId":"03d96201-893b-4d57-9431-c76c810ebb88"},"source":["raw_data = reader.ptb_raw_data(data_dir)\n","train_data, valid_data, test_data, vocab, word_to_id = raw_data"],"execution_count":26,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-26-0e6fc86193a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mraw_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mptb_raw_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_to_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/reader.py\u001b[0m in \u001b[0;36mptb_raw_data\u001b[0;34m(data_path)\u001b[0m\n\u001b[1;32m     72\u001b[0m   \u001b[0mvalid_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ptb.valid.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m   \u001b[0mtest_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ptb.test.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m   \u001b[0mword_to_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_build_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m   \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_file_to_word_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_to_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/reader.py\u001b[0m in \u001b[0;36m_build_vocab\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0m_build_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m   \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/reader.py\u001b[0m in \u001b[0;36m_read_words\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0m_read_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"<eos>\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'gfile'"]}]},{"cell_type":"code","metadata":{"id":"vvkvJexpwAa4","colab_type":"code","colab":{}},"source":["len(train_data)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZKvOOoUowCT2","colab_type":"code","colab":{}},"source":["\n","def id_to_word(id_list):\n","    line = []\n","    for w in id_list:\n","        for word, wid in word_to_id.items():\n","            if wid == w:\n","                line.append(word)\n","    return line            \n","                \n","\n","print(id_to_word(train_data[0:100]))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vCLhup7PwNdg","colab_type":"code","colab":{}},"source":["itera = reader.ptb_iterator(train_data, batch_size, num_steps)\n","first_touple = itera.__next__()\n","x = first_touple[0]\n","y = first_touple[1]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t6iu_1R2wPMn","colab_type":"code","colab":{}},"source":["x.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BMhJpmgZwRSv","colab_type":"code","colab":{}},"source":["x[0:3]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LMNIk1oDwRz4","colab_type":"code","colab":{}},"source":["_input_data = tf.compat.v1.placeholder(tf.int32, [batch_size, num_steps]) #[30#20]\n","_targets = tf.compat.v1.placeholder(tf.int32, [batch_size, num_steps]) #[30#20]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qRIz2kLrwTeP","colab_type":"code","colab":{}},"source":["feed_dict = {_input_data:x, _targets:y}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WIk_yWbSwVsf","colab_type":"code","colab":{}},"source":["session.run(_input_data, feed_dict)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yWYmXJjGwYIn","colab_type":"code","colab":{}},"source":["lstm_cell_l1 = tf.compat.v1.nn.rnn_cell.BasicLSTMCell(hidden_size_l1, forget_bias=0.0)\n","lstm_cell_l2 = tf.compat.v1.nn.rnn_cell.BasicLSTMCell(hidden_size_l2, forget_bias=0.0)\n","stacked_lstm = tf.compat.v1.nn.rnn_cell.MultiRNNCell([lstm_cell_l1, lstm_cell_l2])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vk2TSCTtwZ_n","colab_type":"code","colab":{}},"source":["_initial_state = stacked_lstm.zero_state(batch_size, tf.float32)\n","_initial_state"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SiVocu58wafH","colab_type":"code","colab":{}},"source":["session.run(_initial_state, feed_dict)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3dXaqRypwcHH","colab_type":"code","colab":{}},"source":["embedding_vocab = tf.compat.v1.get_variable(\"embedding_vocab\", [vocab_size, embeding_vector_size])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3ErF-xE3weFP","colab_type":"code","colab":{}},"source":["session.run(tf.compat.v1.global_variables_initializer())\n","session.run(embedding_vocab)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"47ADHTVewhJ_","colab_type":"code","colab":{}},"source":["inputs = tf.nn.embedding_lookup(params=embedding_vocab, ids=_input_data)  #shape=(30, 20, 200) \n","inputs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8SXtaATmwh8J","colab_type":"code","colab":{}},"source":["session.run(inputs[0], feed_dict)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jUwqYrswwksn","colab_type":"code","colab":{}},"source":["outputs, new_state =  tf.compat.v1.nn.dynamic_rnn(stacked_lstm, inputs, initial_state=_initial_state)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8otJzEcKwl93","colab_type":"code","colab":{}},"source":["outputs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mSfKnymYwn7X","colab_type":"code","colab":{}},"source":["session.run(tf.compat.v1.global_variables_initializer())\n","session.run(outputs[0], feed_dict)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RO9TyMBowo0X","colab_type":"code","colab":{}},"source":["output = tf.reshape(outputs, [-1, hidden_size_l2])\n","output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YX5oCoE2wrz5","colab_type":"code","colab":{}},"source":["softmax_w = tf.compat.v1.get_variable(\"softmax_w\", [hidden_size_l2, vocab_size]) #[200x1000]\n","softmax_b = tf.compat.v1.get_variable(\"softmax_b\", [vocab_size]) #[1x1000]\n","logits = tf.matmul(output, softmax_w) + softmax_b\n","prob = tf.nn.softmax(logits)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aBx5-LzrwteI","colab_type":"code","colab":{}},"source":["session.run(tf.compat.v1.global_variables_initializer())\n","output_words_prob = session.run(prob, feed_dict)\n","print(\"shape of the output: \", output_words_prob.shape)\n","print(\"The probability of observing words in t=0 to t=20\", output_words_prob[0:20])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qjQP1mV3wvqW","colab_type":"code","colab":{}},"source":["np.argmax(output_words_prob[0:20], axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EteeXstSwxFO","colab_type":"code","colab":{}},"source":["y[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8nX75dZAwyj2","colab_type":"code","colab":{}},"source":["targ = session.run(_targets, feed_dict) \n","targ[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4SWJxIxkw0f-","colab_type":"code","colab":{}},"source":["loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example([logits], [tf.reshape(_targets, [-1])],[tf.ones([batch_size * num_steps])])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uZ2STXDRw1y-","colab_type":"code","colab":{}},"source":["session.run(loss, feed_dict)[:10]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LjxMpiAYw22H","colab_type":"code","colab":{}},"source":["cost = tf.reduce_sum(input_tensor=loss) / batch_size\n","session.run(tf.compat.v1.global_variables_initializer())\n","session.run(cost, feed_dict)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WUWttdIgxA6H","colab_type":"code","colab":{}},"source":["# Create a variable for the learning rate\n","lr = tf.Variable(0.0, trainable=False)\n","# Create the gradient descent optimizer with our learning rate\n","optimizer = tf.compat.v1.train.GradientDescentOptimizer(lr)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pwGyRBxUxBYL","colab_type":"code","colab":{}},"source":["tvars = tf.compat.v1.trainable_variables()\n","tvars"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QnauseoLxGnu","colab_type":"code","colab":{}},"source":["[v.name for v in tvars]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s0XwvfZixIFo","colab_type":"code","colab":{}},"source":["var_x = tf.compat.v1.placeholder(tf.float32)\n","var_y = tf.compat.v1.placeholder(tf.float32) \n","func_test = 2.0 * var_x * var_x + 3.0 * var_x * var_y\n","session.run(tf.compat.v1.global_variables_initializer())\n","session.run(func_test, {var_x:1.0,var_y:2.0})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YENcMdLbxKQg","colab_type":"code","colab":{}},"source":["var_grad = tf.gradients(ys=func_test, xs=[var_x])\n","session.run(var_grad, {var_x:1.0,var_y:2.0})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eErruat8xQER","colab_type":"code","colab":{}},"source":["var_grad = tf.gradients(ys=func_test, xs=[var_y])\n","session.run(var_grad, {var_x:1.0, var_y:2.0})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"REZO_Er5xUr3","colab_type":"code","colab":{}},"source":["tf.gradients(ys=cost, xs=tvars)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"o-mqSHnoxVD-","colab_type":"code","colab":{}},"source":["grad_t_list = tf.gradients(ys=cost, xs=tvars)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lEKV3F-WxblR","colab_type":"code","colab":{}},"source":["grads, _ = tf.clip_by_global_norm(grad_t_list, max_grad_norm)\n","grads"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XL-AXXXdxcFX","colab_type":"code","colab":{}},"source":["session.run(grads, feed_dict)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LASyPyedxdN3","colab_type":"code","colab":{}},"source":["train_op = optimizer.apply_gradients(zip(grads, tvars))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IslFY8Sgxe5o","colab_type":"code","colab":{}},"source":["session.run(tf.compat.v1.global_variables_initializer())\n","session.run(train_op, feed_dict)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fmv0rALnxgMH","colab_type":"code","colab":{}},"source":["hidden_size_l1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-8pavyjoxhU3","colab_type":"code","colab":{}},"source":["\n","class PTBModel(object):\n","\n","    def __init__(self, action_type):\n","        ######################################\n","        # Setting parameters for ease of use #\n","        ######################################\n","        self.batch_size = batch_size\n","        self.num_steps = num_steps\n","        self.hidden_size_l1 = hidden_size_l1\n","        self.hidden_size_l2 = hidden_size_l2\n","        self.vocab_size = vocab_size\n","        self.embeding_vector_size = embeding_vector_size\n","        ###############################################################################\n","        # Creating placeholders for our input data and expected outputs (target data) #\n","        ###############################################################################\n","        self._input_data = tf.compat.v1.placeholder(tf.int32, [batch_size, num_steps]) #[30#20]\n","        self._targets = tf.compat.v1.placeholder(tf.int32, [batch_size, num_steps]) #[30#20]\n","\n","        ##########################################################################\n","        # Creating the LSTM cell structure and connect it with the RNN structure #\n","        ##########################################################################\n","        # Create the LSTM unit. \n","        # This creates only the structure for the LSTM and has to be associated with a RNN unit still.\n","        # The argument n_hidden(size=200) of BasicLSTMCell is size of hidden layer, that is, the number of hidden units of the LSTM (inside A).\n","        # Size is the same as the size of our hidden layer, and no bias is added to the Forget Gate. \n","        # LSTM cell processes one word at a time and computes probabilities of the possible continuations of the sentence.\n","        lstm_cell_l1 = tf.compat.v1.nn.rnn_cell.BasicLSTMCell(self.hidden_size_l1, forget_bias=0.0)\n","        lstm_cell_l2 = tf.compat.v1.nn.rnn_cell.BasicLSTMCell(self.hidden_size_l2, forget_bias=0.0)\n","        \n","        # Unless you changed keep_prob, this won't actually execute -- this is a dropout wrapper for our LSTM unit\n","        # This is an optimization of the LSTM output, but is not needed at all\n","        if action_type == \"is_training\" and keep_prob < 1:\n","            lstm_cell_l1 = tf.contrib.rnn.DropoutWrapper(lstm_cell_l1, output_keep_prob=keep_prob)\n","            lstm_cell_l2 = tf.contrib.rnn.DropoutWrapper(lstm_cell_l2, output_keep_prob=keep_prob)\n","        \n","        # By taking in the LSTM cells as parameters, the MultiRNNCell function junctions the LSTM units to the RNN units.\n","        # RNN cell composed sequentially of multiple simple cells.\n","        stacked_lstm = tf.compat.v1.nn.rnn_cell.MultiRNNCell([lstm_cell_l1, lstm_cell_l2])\n","\n","        # Define the initial state, i.e., the model state for the very first data point\n","        # It initialize the state of the LSTM memory. The memory state of the network is initialized with a vector of zeros and gets updated after reading each word.\n","        self._initial_state = stacked_lstm.zero_state(batch_size, tf.float32)\n","\n","        ####################################################################\n","        # Creating the word embeddings and pointing them to the input data #\n","        ####################################################################\n","        with tf.device(\"/cpu:0\"):\n","            # Create the embeddings for our input data. Size is hidden size.\n","            embedding = tf.compat.v1.get_variable(\"embedding\", [vocab_size, self.embeding_vector_size])  #[10000x200]\n","            # Define where to get the data for our embeddings from\n","            inputs = tf.nn.embedding_lookup(params=embedding, ids=self._input_data)\n","\n","        # Unless you changed keep_prob, this won't actually execute -- this is a dropout addition for our inputs\n","        # This is an optimization of the input processing and is not needed at all\n","        if action_type == \"is_training\" and keep_prob < 1:\n","            inputs = tf.nn.dropout(inputs, 1 - (keep_prob))\n","\n","        ############################################\n","        # Creating the input structure for our RNN #\n","        ############################################\n","        # Input structure is 20x[30x200]\n","        # Considering each word is represended by a 200 dimentional vector, and we have 30 batchs, we create 30 word-vectors of size [30xx2000]\n","        # inputs = [tf.squeeze(input_, [1]) for input_ in tf.split(1, num_steps, inputs)]\n","        # The input structure is fed from the embeddings, which are filled in by the input data\n","        # Feeding a batch of b sentences to a RNN:\n","        # In step 1,  first word of each of the b sentences (in a batch) is input in parallel.  \n","        # In step 2,  second word of each of the b sentences is input in parallel. \n","        # The parallelism is only for efficiency.  \n","        # Each sentence in a batch is handled in parallel, but the network sees one word of a sentence at a time and does the computations accordingly. \n","        # All the computations involving the words of all sentences in a batch at a given time step are done in parallel. \n","\n","        ####################################################################################################\n","        # Instantiating our RNN model and retrieving the structure for returning the outputs and the state #\n","        ####################################################################################################\n","        \n","        outputs, state = tf.compat.v1.nn.dynamic_rnn(stacked_lstm, inputs, initial_state=self._initial_state)\n","        #########################################################################\n","        # Creating a logistic unit to return the probability of the output word #\n","        #########################################################################\n","        output = tf.reshape(outputs, [-1, self.hidden_size_l2])\n","        softmax_w = tf.compat.v1.get_variable(\"softmax_w\", [self.hidden_size_l2, vocab_size]) #[200x1000]\n","        softmax_b = tf.compat.v1.get_variable(\"softmax_b\", [vocab_size]) #[1x1000]\n","        logits = tf.matmul(output, softmax_w) + softmax_b\n","        logits = tf.reshape(logits, [self.batch_size, self.num_steps, vocab_size])\n","        prob = tf.nn.softmax(logits)\n","        out_words = tf.argmax(input=prob, axis=2)\n","        self._output_words = out_words\n","        #########################################################################\n","        # Defining the loss and cost functions for the model's learning to work #\n","        #########################################################################\n","            \n","\n","        # Use the contrib sequence loss and average over the batches\n","        loss = tf.contrib.seq2seq.sequence_loss(\n","            logits,\n","            self.targets,\n","            tf.ones([batch_size, num_steps], dtype=tf.float32),\n","            average_across_timesteps=False,\n","            average_across_batch=True)\n","    \n","#         loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example([logits], [tf.reshape(self._targets, [-1])],\n","#                                                       [tf.ones([batch_size * num_steps])])\n","        self._cost = tf.reduce_sum(input_tensor=loss)\n","\n","        # Store the final state\n","        self._final_state = state\n","\n","        #Everything after this point is relevant only for training\n","        if action_type != \"is_training\":\n","            return\n","\n","        #################################################\n","        # Creating the Training Operation for our Model #\n","        #################################################\n","        # Create a variable for the learning rate\n","        self._lr = tf.Variable(0.0, trainable=False)\n","        # Get all TensorFlow variables marked as \"trainable\" (i.e. all of them except _lr, which we just created)\n","        tvars = tf.compat.v1.trainable_variables()\n","        # Define the gradient clipping threshold\n","        grads, _ = tf.clip_by_global_norm(tf.gradients(ys=self._cost, xs=tvars), max_grad_norm)\n","        # Create the gradient descent optimizer with our learning rate\n","        optimizer = tf.compat.v1.train.GradientDescentOptimizer(self.lr)\n","        # Create the training TensorFlow Operation through our optimizer\n","        self._train_op = optimizer.apply_gradients(zip(grads, tvars))\n","\n","    # Helper functions for our LSTM RNN class\n","\n","    # Assign the learning rate for this model\n","    def assign_lr(self, session, lr_value):\n","        session.run(tf.compat.v1.assign(self.lr, lr_value))\n","\n","    # Returns the input data for this model at a point in time\n","    @property\n","    def input_data(self):\n","        return self._input_data\n","\n","\n","    \n","    # Returns the targets for this model at a point in time\n","    @property\n","    def targets(self):\n","        return self._targets\n","\n","    # Returns the initial state for this model\n","    @property\n","    def initial_state(self):\n","        return self._initial_state\n","\n","    # Returns the defined Cost\n","    @property\n","    def cost(self):\n","        return self._cost\n","\n","    # Returns the final state for this model\n","    @property\n","    def final_state(self):\n","        return self._final_state\n","    \n","    # Returns the final output words for this model\n","    @property\n","    def final_output_words(self):\n","        return self._output_words\n","    \n","    # Returns the current learning rate for this model\n","    @property\n","    def lr(self):\n","        return self._lr\n","\n","    # Returns the training operation defined for this model\n","    @property\n","    def train_op(self):\n","        return self._train_op"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GtXLw6WoxloX","colab_type":"code","colab":{}},"source":["def run_one_epoch(session, m, data, eval_op, verbose=False):\n","\n","    #Define the epoch size based on the length of the data, batch size and the number of steps\n","    epoch_size = ((len(data) // m.batch_size) - 1) // m.num_steps\n","    start_time = time.time()\n","    costs = 0.0\n","    iters = 0\n","\n","    state = session.run(m.initial_state)\n","    \n","    #For each step and data point\n","    for step, (x, y) in enumerate(reader.ptb_iterator(data, m.batch_size, m.num_steps)):\n","        \n","        #Evaluate and return cost, state by running cost, final_state and the function passed as parameter\n","        cost, state, out_words, _ = session.run([m.cost, m.final_state, m.final_output_words, eval_op],\n","                                     {m.input_data: x,\n","                                      m.targets: y,\n","                                      m.initial_state: state})\n","\n","        #Add returned cost to costs (which keeps track of the total costs for this epoch)\n","        costs += cost\n","        \n","        #Add number of steps to iteration counter\n","        iters += m.num_steps\n","\n","        if verbose and step % (epoch_size // 10) == 10:\n","            print(\"Itr %d of %d, perplexity: %.3f speed: %.0f wps\" % (step , epoch_size, np.exp(costs / iters), iters * m.batch_size / (time.time() - start_time)))\n","\n","    # Returns the Perplexity rating for us to keep track of how the model is evolving\n","    return np.exp(costs / iters)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e-H2It04x2LA","colab_type":"code","colab":{}},"source":["# Reads the data and separates it into training data, validation data and testing data\n","raw_data = reader.ptb_raw_data(data_dir)\n","train_data, valid_data, test_data, _, _ = raw_data\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"O9nqDCzwx6CY","colab_type":"code","colab":{}},"source":["# Initializes the Execution Graph and the Session\n","with tf.Graph().as_default(), tf.compat.v1.Session() as session:\n","    initializer = tf.compat.v1.random_uniform_initializer(-init_scale, init_scale)\n","    \n","    # Instantiates the model for training\n","    # tf.variable_scope add a prefix to the variables created with tf.get_variable\n","    with tf.compat.v1.variable_scope(\"model\", reuse=None, initializer=initializer):\n","        m = PTBModel(\"is_training\")\n","        \n","    # Reuses the trained parameters for the validation and testing models\n","    # They are different instances but use the same variables for weights and biases, they just don't change when data is input\n","    with tf.compat.v1.variable_scope(\"model\", reuse=True, initializer=initializer):\n","        mvalid = PTBModel(\"is_validating\")\n","        mtest = PTBModel(\"is_testing\")\n","\n","    #Initialize all variables\n","    tf.compat.v1.global_variables_initializer().run()\n","\n","    for i in range(max_epoch):\n","        # Define the decay for this epoch\n","        lr_decay = decay ** max(i - max_epoch_decay_lr, 0.0)\n","        \n","        # Set the decayed learning rate as the learning rate for this epoch\n","        m.assign_lr(session, learning_rate * lr_decay)\n","\n","        print(\"Epoch %d : Learning rate: %.3f\" % (i + 1, session.run(m.lr)))\n","        \n","        # Run the loop for this epoch in the training model\n","        train_perplexity = run_one_epoch(session, m, train_data, m.train_op, verbose=True)\n","        print(\"Epoch %d : Train Perplexity: %.3f\" % (i + 1, train_perplexity))\n","        \n","        # Run the loop for this epoch in the validation model\n","        valid_perplexity = run_one_epoch(session, mvalid, valid_data, tf.no_op())\n","        print(\"Epoch %d : Valid Perplexity: %.3f\" % (i + 1, valid_perplexity))\n","    \n","    # Run the loop in the testing model to see how effective was our training\n","    test_perplexity = run_one_epoch(session, mtest, test_data, tf.no_op())\n","    \n","    print(\"Test Perplexity: %.3f\" % test_perplexity)"],"execution_count":null,"outputs":[]}]}