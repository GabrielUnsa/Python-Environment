{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SpellChecker - Seq2Seq - TFv2.0 .ipynb","provenance":[{"file_id":"1wrqaZBmZnbtqv75cfVIS4lRmZSzxq3Zb","timestamp":1582665850301}],"collapsed_sections":[],"authorship_tag":"ABX9TyP4fo7747yVfYC1pHK26NpI"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"TPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"nCe5i45hrP65","colab_type":"text"},"source":["Code:  https://github.com/Currie32/Spell-Checker/blob/master/SpellChecker.ipynb\n","\n","\n","Formato Texto: Hace referencias a los archivos de texto plano o texto enriquecido (.rtf por ejemplo)"]},{"cell_type":"code","metadata":{"id":"0dC_cB4OcJ3o","colab_type":"code","outputId":"f6e728dc-1fba-4b27-9ca5-ab644bf81914","executionInfo":{"status":"ok","timestamp":1582666221217,"user_tz":180,"elapsed":5336,"user":{"displayName":"Gabriel Marmanillo","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBY1MrN1cwAk4nGEl7P9z9WrmYm3KDiq7fk0VbyJw=s64","userId":"07923766545187461618"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["#!pip install tensorflow==2.1\n","import tensorflow as tf\n","print(tf.__version__)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["2.1.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"og0JKkI06em6","colab_type":"text"},"source":["#***Importacion de Librerias***"]},{"cell_type":"code","metadata":{"id":"vYS-P1iWr34-","colab_type":"code","colab":{}},"source":["'''\n","* Importacion de Librerias necesarias para la ejecucion\n","'''\n","import pandas as pd\n","import numpy as np\n","import os\n","from os import listdir\n","from os.path import isfile, join\n","from collections import namedtuple\n","from tensorflow.python.layers.core import Dense\n","from tensorflow.python.ops.rnn_cell_impl import _zero_state_tensors\n","import time\n","import re\n","from sklearn.model_selection import train_test_split\n","from google.colab import drive"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6qfO8PWA6SEb","colab_type":"text"},"source":["#***Lectura del Corpus***"]},{"cell_type":"code","metadata":{"id":"CbDr-ChiwPcX","colab_type":"code","outputId":"38763a7e-9be4-4e08-ec05-b46a1a7f1cfe","executionInfo":{"status":"ok","timestamp":1582666252256,"user_tz":180,"elapsed":20406,"user":{"displayName":"Gabriel Marmanillo","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBY1MrN1cwAk4nGEl7P9z9WrmYm3KDiq7fk0VbyJw=s64","userId":"07923766545187461618"}},"colab":{"base_uri":"https://localhost:8080/","height":127}},"source":["'''\n","* Monta la unidad drive\n","'''\n","drive.mount('/content/gdrive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LosDfIHpuWkn","colab_type":"code","colab":{}},"source":["'''\n","* Precondicion: Los libros deben estar en formato texto\n","* Input:  Directorio del libro\n","* Output: Texto del Libro\n","* Postcondicion: EL tipo de dato de book es String, lo cual debemos almacenar o utilizar ya que se reutiliza la variable\n","'''\n","def load_book(path):\n","    input_file = os.path.join(path)\n","    with open(input_file) as f:\n","        book = f.read()\n","    return book"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NQme7Hsdx0m5","colab_type":"code","outputId":"a1363159-f726-4b7b-b3fc-28b26adfa861","executionInfo":{"status":"ok","timestamp":1582666258566,"user_tz":180,"elapsed":729,"user":{"displayName":"Gabriel Marmanillo","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBY1MrN1cwAk4nGEl7P9z9WrmYm3KDiq7fk0VbyJw=s64","userId":"07923766545187461618"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["'''\n","* Precondicion: En el directorio debe exisitir y contener archivos en formato texto\n","* Input:  Directorio donde se encuentra los libros\n","* Output: Vector que contiene rutas de libros\n","* Postcondicion: Solo contiene la ruta para el uso constante de ello\n","Nota: Podemos reemplazar esta funcion con el ls usualmente utilizada\n","'''\n","path = '/content/gdrive/My Drive/DataSets/Corpus/'\n","book_files = [f for f in listdir(path) if isfile(join(path, f))]\n","print(book_files)\n","#book_files = book_files[1:]\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["['Corpus.txt']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"toqbsdmq4Re_","colab_type":"text"},"source":["**NOTA** *Probar que ocurre si hay pdf, csv, entre otros formatos, de texto como se ve que toma?*"]},{"cell_type":"code","metadata":{"id":"COIIXOnvuWkt","colab_type":"code","colab":{}},"source":["'''\n","* Precondicion: Debe haber almenos un libro en formato texto\n","* Input:  Listado de directorio previamente obtenido\n","* Output: Vector de libros (String que contiene los libros)\n","* Postcondicion: Los textos son string, se puede realizar cualquier operacion que involucre de formato string\n","Nota: Si usamos ls, esta funcion nos permitira ahorrar el concatenamiento de la ruta y el directorio, siendo el codigo mas legible\n","'''\n","books = []\n","for book in book_files:\n","    books.append(load_book(path+book))\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NIxoO4OCuWkx","colab_type":"code","outputId":"3ffdc962-df1c-43a4-b972-f58908826052","executionInfo":{"status":"ok","timestamp":1582666266143,"user_tz":180,"elapsed":736,"user":{"displayName":"Gabriel Marmanillo","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBY1MrN1cwAk4nGEl7P9z9WrmYm3KDiq7fk0VbyJw=s64","userId":"07923766545187461618"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["'''\n","* Precondicion: Debe haber ejecutado el proceso anterior y tener almenos un libro en el vector\n","* Input:  Vector de libros\n","* Output: Cantidad de palabras en ese libro\n","* Postcondicion: Ninguna\n","Nota: Este modulo nos ayudará a tener cierto control con la base de datos y la cantidad de palabras que estamos usando \n","'''\n","for i in range(len(books)):\n","    print(\"Hay {} cantidad de palabras en el libro {}.\".format(len(books[i].split()), book_files[i]))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Hay 16559 cantidad de palabras en el libro Corpus.txt.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1rBxnLmbuWk2","colab_type":"code","outputId":"84d96f21-4d2d-4684-bda5-0068ec384e44","executionInfo":{"status":"ok","timestamp":1582666268073,"user_tz":180,"elapsed":740,"user":{"displayName":"Gabriel Marmanillo","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBY1MrN1cwAk4nGEl7P9z9WrmYm3KDiq7fk0VbyJw=s64","userId":"07923766545187461618"}},"colab":{"base_uri":"https://localhost:8080/","height":55}},"source":["'''\n","Vemos que obtenemos como salida de un cierto libro\n","'''\n","books[0][:500]"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Me gusta el navegar en kayak.Los kiwis son una excelente fuente de vitamina C.\\n\\nPAPELES QUE A LA MUERTE DE GÜEMES\\nQUEDARON EN PODER DE LA FAMILIA\\n\\nLa primera información, apoyada en documentos, con que\\ncontamos sobre la suerte corrida por los \"papeles de Güemes\"\\nnos la dan las cartas, hoy en nuestro poder, que Martín, el hijo\\nprimogénito de Güemes, mientras acompañaba a algunos exilados\\ntíos maternos en el Perú, escribió en distintas oportunidades\\ndesde el Cerro de Pasco a su hermano Luis, de re'"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"7stUxGoR6D58","colab_type":"text"},"source":["# ***Preparacion del Corpus***"]},{"cell_type":"code","metadata":{"id":"HWlJT7jNuWk7","colab_type":"code","colab":{}},"source":["'''\n","* Precondicion: Un texto netamente puro\n","* Input:  El texto = El libro\n","* Output: Libro limpio sin caracteres especiales, tabulaciones, entre otros.\n","* Postcondicion: Ninguna\n","NOTA: Podemos usar el limpiador que utiliza el Dr. Xamena\n","'''\n","def clean_text(text):\n","    text = re.sub(r'\\n', ' ', text) \n","    text = re.sub(r'[{}@_*>()\\\\#%+=\\[\\]]','', text)\n","    text = re.sub('a0','', text)\n","    text = re.sub('\\'92t','\\'t', text)\n","    text = re.sub('\\'92s','\\'s', text)\n","    text = re.sub('\\'92m','\\'m', text)\n","    text = re.sub('\\'92ll','\\'ll', text)\n","    text = re.sub('\\'91','', text)\n","    text = re.sub('\\'92','', text)\n","    text = re.sub('\\'93','', text)\n","    text = re.sub('\\'94','', text)\n","    text = re.sub('\\.','. ', text)\n","    text = re.sub('\\!','! ', text)\n","    text = re.sub('\\?','? ', text)\n","    text = re.sub(' +',' ', text)\n","    return text"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-xj5b-ciuWk-","colab_type":"code","colab":{}},"source":["'''\n","* Precondicion: Array de libros sin limpiar\n","* Input: Array de libros\n","* Output: Array de libros sin caracteres especiales, etc., es decir, limpios.\n","'''\n","clean_books = []\n","for book in books:\n","    clean_books.append(clean_text(book))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"upxZLZM3uWlC","colab_type":"code","outputId":"73c6aea8-d8cc-4228-d549-734591355cf6","executionInfo":{"status":"ok","timestamp":1582666273433,"user_tz":180,"elapsed":803,"user":{"displayName":"Gabriel Marmanillo","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBY1MrN1cwAk4nGEl7P9z9WrmYm3KDiq7fk0VbyJw=s64","userId":"07923766545187461618"}},"colab":{"base_uri":"https://localhost:8080/","height":55}},"source":["'''\n","* Controlamos un texto alatorio como se ve para seguir refinando la limpieza\n","'''\n","clean_books[0][500:1500]"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'reso éste en Salta desde el mismo lugar. Gracias a las del año 1847, sabemos lo siguiente: que, tras la muerte de Güemes 1821 y la de su viuda, doña Carmen Puch 1822, por minoridad de sus hijos, nacidos en 1817 y 1819 respectivamente, dichos papeles quedaron en manos de familiares cercanos; que quien después los tuvo en su poder y los ordenó fue el doctor José Redhead, el conocido médico de Güemes y de Belgrano, de gran prestigio y larga residencia en Salta “él fue el archivo de todo\", según tales cartas; que, muerto el nombrado facultativo el 3 de junio de 1846, Martín Güemes y Puch, en procura de que tan valiosos documentos prestaran pronta utilidad a la Historia, pidió una y otra vez a Luis que recogiera y le remitiera \"los papeles relativos a nuestro padre -decía- que debió dejar el doctor Redhead y algunas apuntaciones hechas por este sabio amigo\" , para, con don Manuel Puch, fío de ambos, hacer preparar una \"biografía\", y, concluida que fuese ésta, el último de los nombrados los '"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"MdziqbgYuWlH","colab_type":"code","colab":{}},"source":["'''\n","* Precondicion: Contar con un texto casi puro para codificar los caracteres en enteros para la aplicacion del modelo\n","* Input: Texto de libros limpios\n","* Output: Vocabulario codificado con entero\n","* Postcondicion: vocabulario(diccionario) codificado en entero, sin tokens especiales\n","'''\n","vocab_to_int = {}\n","count = 0\n","for book in clean_books:\n","    for character in book:\n","        if character not in vocab_to_int:\n","            vocab_to_int[character] = count\n","            count += 1"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CViosj8j-4WA","colab_type":"code","colab":{}},"source":["'''\n","* Precondicion: vocabulario(diccionario) codificado con enteros\n","* Input: Vocabulario codificado con enteros sin caracteres especiales \n","* Output: Vocabulario codificado con enteros con caracteres especiales\n","* Postcondicion: Ninguna\n","Nota: Estos tokens especiales son especificamente para modelos seq2seq para el entrenamiento del mismo\n","'''\n","codes = ['<PAD>','<EOS>','<GO>','k']\n","for code in codes:\n","    vocab_to_int[code] = count\n","    count += 1"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tVGImZq5uWlK","colab_type":"code","outputId":"784c21a4-20d9-4583-f042-e12a16e6bc97","executionInfo":{"status":"ok","timestamp":1582666290261,"user_tz":180,"elapsed":731,"user":{"displayName":"Gabriel Marmanillo","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBY1MrN1cwAk4nGEl7P9z9WrmYm3KDiq7fk0VbyJw=s64","userId":"07923766545187461618"}},"colab":{"base_uri":"https://localhost:8080/","height":73}},"source":["'''\n","Muestra el vocabulario ordenado\n","'''\n","print(\"El vocabulario contiene {} caracteres.\".format(len(vocab_to_int)))\n","print(sorted(vocab_to_int))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["El vocabulario contiene 101 caracteres.\n","['\\t', ' ', '!', '\"', '$', \"'\", ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '<EOS>', '<GO>', '<PAD>', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '¡', 'ª', '«', 'º', '»', '¿', 'Á', 'É', 'Í', 'Ñ', 'Ó', 'Ú', 'Ü', 'á', 'é', 'í', 'ñ', 'ó', 'ú', 'ü', '—', '“', '”']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ATJxC3tGuWlO","colab_type":"code","outputId":"ecfa7239-4fb7-41ef-f86b-18a8d0f8fe84","executionInfo":{"status":"ok","timestamp":1582666292481,"user_tz":180,"elapsed":794,"user":{"displayName":"Gabriel Marmanillo","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBY1MrN1cwAk4nGEl7P9z9WrmYm3KDiq7fk0VbyJw=s64","userId":"07923766545187461618"}},"colab":{"base_uri":"https://localhost:8080/","height":73}},"source":["'''\n","* Precondicion: Vocabulario(diccionario) codificado en enteros con caracteres especiales \n","* Input: Vocabulario(diccionario) codificado en enteros con caracteres especiales\n","* Output: Vocabulario(diccionario)\n","* Postcondicion: Ninguna\n","'''\n","int_to_vocab = {}\n","for character, value in vocab_to_int.items():\n","    int_to_vocab[value] = character\n","print(\"El vocabulario contiene {} caracteres.\".format(len(int_to_vocab)))\n","print(sorted(int_to_vocab))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["El vocabulario contiene 101 caracteres.\n","[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OsOYxfgCuWlQ","colab_type":"code","outputId":"b5380744-716f-4421-8f4e-011481d23ba8","executionInfo":{"status":"ok","timestamp":1582666302942,"user_tz":180,"elapsed":862,"user":{"displayName":"Gabriel Marmanillo","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBY1MrN1cwAk4nGEl7P9z9WrmYm3KDiq7fk0VbyJw=s64","userId":"07923766545187461618"}},"colab":{"base_uri":"https://localhost:8080/","height":145}},"source":["'''\n","* Precondicion: Libros limpios  \n","* Input: Array con libros limpios\n","* Output: Array de oraciones de los libros\n","* Postcondicion: Ninguna\n","Nota: A cada oracion del texto del libro lo convierte en oraciones separadas por la funcion split.\n","'''\n","sentences = []\n","for book in clean_books:\n","    for sentence in book.split('. '):\n","        sentences.append(sentence + '.')\n","print(\"Hay {} oraciones.\".format(len(sentences)))\n","\n","#Ejemplo\n","sentences[5:10]"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Hay 916 oraciones.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["['Si bien unas y otras apuntaciones carecen de firma, el cotejo de su letra con la de la carta antedicha nos cerciora de que las tres piezas salieron de la misma pluma.',\n"," 'Aun sin la base documental precedente, los nombrados Manuel y Martín, en cumplimiento de su propósito, hicieron publicar en Lima, en ese mismo año 1847, una Biografía del general don Martín Güemes, fundándose en los recuerdos personales del primero \"lo que un proscripto ha conservado en la memoria\",salvo nna carta autógrafa de Pueyrredón a Güemes que dicen poseer —no sabemos cómo la hubieron — y que incluyen, si bien atribuyéndosela erradamente a San Martín, por estar firmada “J», Martín\".',\n"," 'A falta del original, tomándola de dicha Biografía, la reproducimos bajo número 189 en el \"Epistolario\".',\n"," 'A partir de entonces! pasan dos décadas sin que trascienda novedad alguna con respecto a los papeles de Güemes.',\n"," 'La explicación es la siguiente: Cuando en 1832 los Puch, por ser unitarios, emigraron al Perú, como hemos dicho, llevaron a Luis, a la sazón prohijado por los gauchos y de trece años de edad.']"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","metadata":{"id":"oZSrdxaKuWlV","colab_type":"code","colab":{}},"source":["'''\n","* Precondiciones: Array de oraciones\n","* Input: Array de oraciones\n","* Output: Array de oraciones, codificada los caracteres en enteros\n","* Postcondiciones: Ninguna\n","'''\n","int_sentences = []\n","\n","for sentence in sentences:\n","    int_sentence = []\n","    for character in sentence:\n","        int_sentence.append(vocab_to_int[character])\n","    int_sentences.append(int_sentence)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"I3uhiRzQuWlb","colab_type":"code","outputId":"a320be76-7994-4d8f-b915-3f9b315d5261","executionInfo":{"status":"ok","timestamp":1582666307463,"user_tz":180,"elapsed":689,"user":{"displayName":"Gabriel Marmanillo","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBY1MrN1cwAk4nGEl7P9z9WrmYm3KDiq7fk0VbyJw=s64","userId":"07923766545187461618"}},"colab":{"base_uri":"https://localhost:8080/","height":297}},"source":["'''\n","* Precondiciones: oraciones codificada en enteros\n","* Input: Array de oraciones codificada en enteros\n","* Output: DataFrame de las oraciones\n","* Postcondiciones: Dataframe hecho en pandas para poder sacar facilmente estadisticos\n","'''\n","lengths = []\n","for sentence in int_sentences:\n","    lengths.append(len(sentence))\n","lengths = pd.DataFrame(lengths, columns=[\"counts\"])\n","lengths.describe()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>counts</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>916.000000</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>104.156114</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>130.711758</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>1.000000</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>4.000000</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>61.000000</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>149.000000</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>1189.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["            counts\n","count   916.000000\n","mean    104.156114\n","std     130.711758\n","min       1.000000\n","25%       4.000000\n","50%      61.000000\n","75%     149.000000\n","max    1189.000000"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"code","metadata":{"id":"taUQESkzuWlh","colab_type":"code","outputId":"f5cd5eb8-1d94-4992-b380-e1cb8bfe941c","executionInfo":{"status":"ok","timestamp":1582666309857,"user_tz":180,"elapsed":739,"user":{"displayName":"Gabriel Marmanillo","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBY1MrN1cwAk4nGEl7P9z9WrmYm3KDiq7fk0VbyJw=s64","userId":"07923766545187461618"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["'''\n","* Precondicion: array de oraciones codificadas en entero \n","* Input: array de oraciones codificadas en entero\n","* Output: cantidad de oraciones que se usaran para el entrenamiento y la prueba\n","* Postcondiciones: Ninguna\n","Nota: Filtraremos oraciones cortas o muy larga para el modelo este es la ventana que se puede modificar.\n","'''\n","max_length = 92\n","min_length = 10\n","\n","good_sentences = []\n","\n","for sentence in int_sentences:\n","    if len(sentence) <= max_length and len(sentence) >= min_length:\n","        good_sentences.append(sentence)\n","\n","print(\"Usaremos {} para entrenar y probar nuestro modelo.\".format(len(good_sentences)))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Usaremos 293 para entrenar y probar nuestro modelo.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LBzlifUauWll","colab_type":"code","outputId":"0781b8fa-9278-4993-c13f-fefe805cf030","executionInfo":{"status":"ok","timestamp":1582666320666,"user_tz":180,"elapsed":723,"user":{"displayName":"Gabriel Marmanillo","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBY1MrN1cwAk4nGEl7P9z9WrmYm3KDiq7fk0VbyJw=s64","userId":"07923766545187461618"}},"colab":{"base_uri":"https://localhost:8080/","height":53}},"source":["'''\n","Tomaremos solo el 0.15% de las oraciones para probar y el resto sera para entrenamiento\n","'''\n","training, testing = train_test_split(good_sentences, test_size = 0.15, random_state = 2)\n","\n","print(\"Oraciones para Entrenamiento:\", len(training))\n","print(\"Oraciones para Prueba:\", len(testing))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Oraciones para Entrenamiento: 249\n","Oraciones para Prueba: 44\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"NCSUFOy5uWlo","colab_type":"code","colab":{}},"source":["'''\n","Ordenamos las oraciones para que el entrenamiento sea mas rapido y la ventana vaya creciendo de a poco\n","'''\n","training_sorted = []\n","testing_sorted = []\n","\n","for i in range(min_length, max_length+1):\n","    for sentence in training:\n","        if len(sentence) == i:\n","            training_sorted.append(sentence)\n","    for sentence in testing:\n","        if len(sentence) == i:\n","            testing_sorted.append(sentence)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xAv9-LJXuWlq","colab_type":"code","outputId":"1ebb66b8-2e4c-45ce-9fd3-459e2e020d8b","executionInfo":{"status":"ok","timestamp":1582666324504,"user_tz":180,"elapsed":737,"user":{"displayName":"Gabriel Marmanillo","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBY1MrN1cwAk4nGEl7P9z9WrmYm3KDiq7fk0VbyJw=s64","userId":"07923766545187461618"}},"colab":{"base_uri":"https://localhost:8080/","height":107}},"source":["'''\n","Veremos que las oraciones esten ordenando correctamente\n","'''\n","for i in range(5):\n","    print(\"Oracion:  \" + str( training_sorted[i] ) + \" Longitud: \" + str( len(training_sorted[i]) ) )"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Oracion:  [1, 8, 2, 43, 4, 1, 2, 30, 22, 14] Longitud: 10\n","Oracion:  [42, 2, 33, 1, 5, 6, 17, 9, 16, 5, 14] Longitud: 11\n","Oracion:  [42, 2, 1, 9, 2, 25, 16, 6, 16, 5, 48, 14] Longitud: 12\n","Oracion:  [23, 4, 20, 47, 16, 5, 2, 7, 51, 16, 5, 14] Longitud: 12\n","Oracion:  [23, 4, 20, 47, 16, 5, 2, 7, 51, 16, 5, 14] Longitud: 12\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tlO-kjo-uWlt","colab_type":"code","colab":{}},"source":["'''\n","* Precondicion: oraciones y una probabilidad menor a 1 previamentes definidas\n","* Input: Array de oraciones codificadas y una probabilidad\n","* Output: Arrya de oraciones mal escritas\n","* Postcondiciones:  Ninguna\n","Crea errores en las oraciones, si la probabilidad es menor a threshold:\n","* si es mayor al 67% quita un caracter\n","  sino reordena un caracter\n","* con el 33% restante agrega un caracter \n","'''\n","\n","letters = ['a','b','c','d','e','f','g','h','i','j','k','l','m',\n","           'n','o','p','q','r','s','t','u','v','w','x','y','z',]\n","\n","def noise_maker(sentence, threshold):\n","    noisy_sentence = []\n","    i = 0\n","    while i < len(sentence):\n","        random = np.random.uniform(0,1,1)\n","        if random < threshold:\n","            noisy_sentence.append(sentence[i])\n","        else:\n","            new_random = np.random.uniform(0,1,1)\n","            if new_random > 0.67:\n","                if i == (len(sentence) - 1):\n","                    continue\n","                else:\n","                    noisy_sentence.append(sentence[i+1])\n","                    noisy_sentence.append(sentence[i])\n","                    i += 1\n","            elif new_random < 0.33:\n","                random_letter = np.random.choice(letters, 1)[0]\n","                noisy_sentence.append(vocab_to_int[random_letter])\n","                noisy_sentence.append(sentence[i])\n","            else:\n","                pass     \n","        i += 1\n","    return noisy_sentence"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"keT2UHGIuWlw","colab_type":"code","outputId":"f37bb807-f3a2-4a3e-8c6c-b44973e9dded","executionInfo":{"status":"ok","timestamp":1582666331863,"user_tz":180,"elapsed":729,"user":{"displayName":"Gabriel Marmanillo","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBY1MrN1cwAk4nGEl7P9z9WrmYm3KDiq7fk0VbyJw=s64","userId":"07923766545187461618"}},"colab":{"base_uri":"https://localhost:8080/","height":287}},"source":["'''\n","Muestras algunas oraciones creadas\n","'''\n","threshold = 0.9\n","for sentence in training_sorted[:5]:\n","    print(sentence)\n","    print(noise_maker(sentence, threshold))\n","    print()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[1, 8, 2, 43, 4, 1, 2, 30, 22, 14]\n","[1, 8, 2, 4, 1, 30, 2, 22, 14]\n","\n","[42, 2, 33, 1, 5, 6, 17, 9, 16, 5, 14]\n","[42, 2, 33, 1, 5, 6, 17, 9, 16, 5, 14]\n","\n","[42, 2, 1, 9, 2, 25, 16, 6, 16, 5, 48, 14]\n","[42, 2, 1, 9, 2, 25, 16, 6, 16, 5, 48, 14]\n","\n","[23, 4, 20, 47, 16, 5, 2, 7, 51, 16, 5, 14]\n","[3, 23, 4, 20, 47, 16, 5, 2, 7, 51, 5, 14]\n","\n","[23, 4, 20, 47, 16, 5, 2, 7, 51, 16, 5, 14]\n","[23, 4, 20, 5, 47, 16, 44, 5, 2, 7, 51, 16, 5, 14]\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"pIt_OAvpWCwK","colab_type":"text"},"source":["# **Construccion del Modelo**"]},{"cell_type":"code","metadata":{"id":"V96voLBxWIJz","colab_type":"code","colab":{}},"source":["'''\n","Crea los marcadores de entrada del modelo\n","Nota en tensorFlow 2.x ya estos marcadores estan obsolotes y no se usan\n","'''\n","\n","def model_inputs():    \n","    with tf.name_scope('inputs'): #Definicion del operador input\n","        inputs = tf.placeholder(tf.int32, [None, None], name='inputs') #crea la variable que toman el valor de la letra y el entero que lo codifica\n","    with tf.name_scope('targets'): #Definicion del operador targents\n","        targets = tf.placeholder(tf.int32, [None, None], name='targets') #crea la variable que toman el valor de la oracion con en numero???\n","    #Estadisticos que se tomaran para el entrenamiento?\n","    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n","    inputs_length = tf.placeholder(tf.int32, (None,), name='inputs_length')\n","    targets_length = tf.placeholder(tf.int32, (None,), name='targets_length')\n","    max_target_length = tf.reduce_max(targets_length, name='max_target_len')\n","\n","    return inputs, targets, keep_prob, inputs_length, targets_length, max_target_length"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HU9afX8_uWl6","colab_type":"code","colab":{}},"source":["'''\n","* Precondiciones:\n","* Input:\n","* Output:\n","* Postcondiciones:\n","'''\n","def process_encoding_input(targets, vocab_to_int, batch_size):\n","    with tf.name_scope(\"process_encoding\"):\n","        ending = tf.strided_slice(targets, [0, 0], [batch_size, -1], [1, 1])\n","        dec_input = tf.concat([tf.fill([batch_size, 1], vocab_to_int['<GO>']), ending], 1)\n","    return dec_input"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7oK5ysL5uWl-","colab_type":"code","colab":{}},"source":["'''\n","* Precondiciones:\n","* Input:\n","* Output:\n","* Postcondiciones:\n","'''\n","def encoding_layer(rnn_size, sequence_length, num_layers, rnn_inputs, keep_prob, direction): \n","    if direction == 1:\n","        with tf.name_scope(\"RNN_Encoder_Cell_1D\"):\n","            for layer in range(num_layers):\n","                with tf.variable_scope('encoder_{}'.format(layer)):\n","                    lstm = tf.contrib.rnn.LSTMCell(rnn_size)\n","\n","                    drop = tf.contrib.rnn.DropoutWrapper(lstm, \n","                                                         input_keep_prob = keep_prob)\n","\n","                    enc_output, enc_state = tf.nn.dynamic_rnn(drop, \n","                                                              rnn_inputs,\n","                                                              sequence_length,\n","                                                              dtype=tf.float32)\n","\n","            return enc_output, enc_state\n","          \n","    if direction == 2:\n","        with tf.name_scope(\"RNN_Encoder_Cell_2D\"):\n","            for layer in range(num_layers):\n","                with tf.variable_scope('encoder_{}'.format(layer)):\n","                    cell_fw = tf.contrib.rnn.LSTMCell(rnn_size)\n","                    cell_fw = tf.contrib.rnn.DropoutWrapper(cell_fw, \n","                                                            input_keep_prob = keep_prob)\n","\n","                    cell_bw = tf.contrib.rnn.LSTMCell(rnn_size)\n","                    cell_bw = tf.contrib.rnn.DropoutWrapper(cell_bw, \n","                                                            input_keep_prob = keep_prob)\n","\n","                    enc_output, enc_state = tf.nn.bidirectional_dynamic_rnn(cell_fw, \n","                                                                            cell_bw, \n","                                                                            rnn_inputs,\n","                                                                            sequence_length,\n","                                                                            dtype=tf.float32)\n","            enc_output = tf.concat(enc_output,2)\n","            return enc_output, enc_state[0]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gezwezw-uWmD","colab_type":"code","colab":{}},"source":["'''\n","* Precondiciones:\n","* Input:\n","* Output:\n","* Postcondiciones:\n","'''\n","def training_decoding_layer(dec_embed_input, targets_length, dec_cell, initial_state, output_layer, \n","                            vocab_size, max_target_length):\n","    with tf.name_scope(\"Training_Decoder\"):\n","        training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_embed_input,\n","                                                            sequence_length=targets_length,\n","                                                            time_major=False)\n","\n","        training_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n","                                                           training_helper,\n","                                                           initial_state,\n","                                                           output_layer) \n","\n","        training_logits, _ = tf.contrib.seq2seq.dynamic_decode(training_decoder,\n","                                                               output_time_major=False,\n","                                                               impute_finished=True,\n","                                                               maximum_iterations=max_target_length)\n","        return training_logits"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vf5784iLuWmG","colab_type":"code","colab":{}},"source":["'''\n","* Precondiciones:\n","* Input:\n","* Output:\n","* Postcondiciones:\n","'''\n","def inference_decoding_layer(embeddings, start_token, end_token, dec_cell, initial_state, output_layer,\n","                             max_target_length, batch_size):\n","    with tf.name_scope(\"Inference_Decoder\"):\n","        start_tokens = tf.tile(tf.constant([start_token], dtype=tf.int32), [batch_size], name='start_tokens')\n","\n","        inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embeddings,\n","                                                                    start_tokens,\n","                                                                    end_token)\n","\n","        inference_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n","                                                            inference_helper,\n","                                                            initial_state,\n","                                                            output_layer)\n","\n","        inference_logits, _ = tf.contrib.seq2seq.dynamic_decode(inference_decoder,\n","                                                                output_time_major=False,\n","                                                                impute_finished=True,\n","                                                                maximum_iterations=max_target_length)\n","\n","        return inference_logits"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MCJUk0NGuWmJ","colab_type":"code","colab":{}},"source":["'''\n","* Precondiciones:\n","* Input:\n","* Output:\n","* Postcondiciones:\n","'''\n","def decoding_layer(dec_embed_input, embeddings, enc_output, enc_state, vocab_size, inputs_length, targets_length, \n","                   max_target_length, rnn_size, vocab_to_int, keep_prob, batch_size, num_layers, direction):   \n","    with tf.name_scope(\"RNN_Decoder_Cell\"):\n","        for layer in range(num_layers):\n","            with tf.variable_scope('decoder_{}'.format(layer)):\n","                lstm = tf.contrib.rnn.LSTMCell(rnn_size)\n","                dec_cell = tf.contrib.rnn.DropoutWrapper(lstm, \n","                                                         input_keep_prob = keep_prob)\n","    \n","    output_layer = Dense(vocab_size,\n","                         kernel_initializer = tf.truncated_normal_initializer(mean = 0.0, stddev=0.1))\n","    \n","    attn_mech = tf.contrib.seq2seq.BahdanauAttention(rnn_size,\n","                                                  enc_output,\n","                                                  inputs_length,\n","                                                  normalize=False,\n","                                                  name='BahdanauAttention')\n","    \n","    with tf.name_scope(\"Attention_Wrapper\"):\n","        dec_cell = tf.contrib.seq2seq.DynamicAttentionWrapper(dec_cell,\n","                                                              attn_mech,\n","                                                              rnn_size)\n","    \n","    initial_state = tf.contrib.seq2seq.DynamicAttentionWrapperState(enc_state,\n","                                                                    _zero_state_tensors(rnn_size, \n","                                                                                        batch_size, \n","                                                                                        tf.float32))\n","\n","    with tf.variable_scope(\"decode\"):\n","        training_logits = training_decoding_layer(dec_embed_input, \n","                                                  targets_length, \n","                                                  dec_cell, \n","                                                  initial_state,\n","                                                  output_layer,\n","                                                  vocab_size, \n","                                                  max_target_length)\n","    with tf.variable_scope(\"decode\", reuse=True):\n","        inference_logits = inference_decoding_layer(embeddings,  \n","                                                    vocab_to_int['<GO>'], \n","                                                    vocab_to_int['<EOS>'],\n","                                                    dec_cell, \n","                                                    initial_state, \n","                                                    output_layer,\n","                                                    max_target_length,\n","                                                    batch_size)\n","\n","    return training_logits, inference_logits"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WgRVj-TxuWmO","colab_type":"code","colab":{}},"source":["'''\n","* Precondiciones:\n","* Input:\n","* Output:\n","* Postcondiciones:\n","'''\n","def seq2seq_model(inputs, targets, keep_prob, inputs_length, targets_length, max_target_length, \n","                  vocab_size, rnn_size, num_layers, vocab_to_int, batch_size, embedding_size, direction):\n","   \n","    enc_embeddings = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1, 1))\n","    enc_embed_input = tf.nn.embedding_lookup(enc_embeddings, inputs)\n","    enc_output, enc_state = encoding_layer(rnn_size, inputs_length, num_layers, \n","                                           enc_embed_input, keep_prob, direction)\n","    \n","    dec_embeddings = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1, 1))\n","    dec_input = process_encoding_input(targets, vocab_to_int, batch_size)\n","    dec_embed_input = tf.nn.embedding_lookup(dec_embeddings, dec_input)\n","    \n","    training_logits, inference_logits  = decoding_layer(dec_embed_input, \n","                                                        dec_embeddings,\n","                                                        enc_output,\n","                                                        enc_state, \n","                                                        vocab_size, \n","                                                        inputs_length, \n","                                                        targets_length, \n","                                                        max_target_length,\n","                                                        rnn_size, \n","                                                        vocab_to_int, \n","                                                        keep_prob, \n","                                                        batch_size,\n","                                                        num_layers,\n","                                                        direction)\n","    \n","    return training_logits, inference_logits"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7sGo_f2duWmQ","colab_type":"code","colab":{}},"source":["'''\n","* Precondiciones:\n","* Input:\n","* Output:\n","* Postcondiciones:\n","'''\n","def pad_sentence_batch(sentence_batch):\n","    max_sentence = max([len(sentence) for sentence in sentence_batch])\n","    return [sentence + [vocab_to_int['<PAD>']] * (max_sentence - len(sentence)) for sentence in sentence_batch]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ptphl44LuWmU","colab_type":"code","colab":{}},"source":["'''\n","* Precondiciones:\n","* Input:\n","* Output:\n","* Postcondiciones:\n","'''\n","def get_batches(sentences, batch_size, threshold):\n","\n","    for batch_i in range(0, len(sentences)//batch_size):\n","        start_i = batch_i * batch_size\n","        sentences_batch = sentences[start_i:start_i + batch_size]\n","        \n","        sentences_batch_noisy = []\n","        for sentence in sentences_batch:\n","            sentences_batch_noisy.append(noise_maker(sentence, threshold))\n","            \n","        sentences_batch_eos = []\n","        for sentence in sentences_batch:\n","            sentence.append(vocab_to_int['<EOS>'])\n","            sentences_batch_eos.append(sentence)\n","            \n","        pad_sentences_batch = np.array(pad_sentence_batch(sentences_batch_eos))\n","        pad_sentences_noisy_batch = np.array(pad_sentence_batch(sentences_batch_noisy))\n","        \n","        pad_sentences_lengths = []\n","        for sentence in pad_sentences_batch:\n","            pad_sentences_lengths.append(len(sentence))\n","        \n","        pad_sentences_noisy_lengths = []\n","        for sentence in pad_sentences_noisy_batch:\n","            pad_sentences_noisy_lengths.append(len(sentence))\n","        \n","        yield pad_sentences_noisy_batch, pad_sentences_batch, pad_sentences_noisy_lengths, pad_sentences_lengths"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ka_labNpuWmY","colab_type":"code","colab":{}},"source":["# Parametros por default\n","epochs = 100\n","batch_size = 128\n","num_layers = 2\n","rnn_size = 512\n","embedding_size = 128\n","learning_rate = 0.0005\n","direction = 2\n","threshold = 0.95\n","keep_probability = 0.75"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ydn1VBvAuWmc","colab_type":"code","colab":{}},"source":["'''\n","* Precondiciones:\n","* Input:\n","* Output:\n","* Postcondiciones:\n","'''\n","def build_graph(keep_prob, rnn_size, num_layers, batch_size, learning_rate, embedding_size, direction):\n","\n","    tf.reset_default_graph()\n","\n","    inputs, targets, keep_prob, inputs_length, targets_length, max_target_length = model_inputs()\n","\n","    training_logits, inference_logits = seq2seq_model(tf.reverse(inputs, [-1]),\n","                                                      targets, \n","                                                      keep_prob,   \n","                                                      inputs_length,\n","                                                      targets_length,\n","                                                      max_target_length,\n","                                                      len(vocab_to_int)+1,\n","                                                      rnn_size, \n","                                                      num_layers, \n","                                                      vocab_to_int,\n","                                                      batch_size,\n","                                                      embedding_size,\n","                                                      direction)\n","\n","    training_logits = tf.identity(training_logits.rnn_output, 'logits')\n","\n","    with tf.name_scope('predictions'):\n","        predictions = tf.identity(inference_logits.sample_id, name='predictions')\n","        tf.summary.histogram('predictions', predictions)\n","    masks = tf.sequence_mask(targets_length, max_target_length, dtype=tf.float32, name='masks')\n","    \n","    with tf.name_scope(\"cost\"):\n","        cost = tf.contrib.seq2seq.sequence_loss(training_logits, \n","                                                targets, \n","                                                masks)\n","        tf.summary.scalar('cost', cost)\n","\n","    with tf.name_scope(\"optimze\"):\n","        optimizer = tf.train.AdamOptimizer(learning_rate)\n","\n","        gradients = optimizer.compute_gradients(cost)\n","        capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\n","        train_op = optimizer.apply_gradients(capped_gradients)\n","\n","    merged = tf.summary.merge_all()    \n","\n","    export_nodes = ['inputs', 'targets', 'keep_prob', 'cost', 'inputs_length', 'targets_length',\n","                    'predictions', 'merged', 'train_op','optimizer']\n","    Graph = namedtuple('Graph', export_nodes)\n","    local_dict = locals()\n","    graph = Graph(*[local_dict[each] for each in export_nodes])\n","\n","    return graph"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_bS7oUsMW-FV","colab_type":"text"},"source":["# **Entrenamiento del Modelo**"]},{"cell_type":"code","metadata":{"scrolled":false,"id":"BfYIQmM0uWmf","colab_type":"code","colab":{}},"source":["'''\n","* Precondiciones:\n","* Input:\n","* Output:\n","* Postcondiciones:\n","'''\n","def train(model, epochs, log_string):\n","\n","    with tf.Session() as sess:\n","        sess.run(tf.global_variables_initializer())\n","\n","        testing_loss_summary = []\n","\n","        iteration = 0\n","        \n","        display_step = 30 \n","        stop_early = 0 \n","        stop = 3 \n","        per_epoch = 3 \n","        testing_check = (len(training_sorted)//batch_size//per_epoch)-1\n","\n","        print()\n","        print(\"Training Model: {}\".format(log_string))\n","\n","        train_writer = tf.summary.FileWriter('./logs/1/train/{}'.format(log_string), sess.graph)\n","        test_writer = tf.summary.FileWriter('./logs/1/test/{}'.format(log_string))\n","\n","        for epoch_i in range(1, epochs+1): \n","            batch_loss = 0\n","            batch_time = 0\n","            \n","            for batch_i, (input_batch, target_batch, input_length, target_length) in enumerate(\n","                    get_batches(training_sorted, batch_size, threshold)):\n","                start_time = time.time()\n","\n","                summary, loss, _ = sess.run([model.merged,\n","                                             model.cost, \n","                                             model.train_op], \n","                                             {model.inputs: input_batch,\n","                                              model.targets: target_batch,\n","                                              model.inputs_length: input_length,\n","                                              model.targets_length: target_length,\n","                                              model.keep_prob: keep_probability})\n","\n","\n","                batch_loss += loss\n","                end_time = time.time()\n","                batch_time += end_time - start_time\n","                train_writer.add_summary(summary, iteration)\n","\n","                iteration += 1\n","\n","                if batch_i % display_step == 0 and batch_i > 0:\n","                    print('Epoch {:>3}/{} Batch {:>4}/{} - Loss: {:>6.3f}, Seconds: {:>4.2f}'\n","                          .format(epoch_i,\n","                                  epochs, \n","                                  batch_i, \n","                                  len(training_sorted) // batch_size, \n","                                  batch_loss / display_step, \n","                                  batch_time))\n","                    batch_loss = 0\n","                    batch_time = 0\n","\n","                if batch_i % testing_check == 0 and batch_i > 0:\n","                    batch_loss_testing = 0\n","                    batch_time_testing = 0\n","                    for batch_i, (input_batch, target_batch, input_length, target_length) in enumerate(\n","                            get_batches(testing_sorted, batch_size, threshold)):\n","                        start_time_testing = time.time()\n","                        summary, loss = sess.run([model.merged,\n","                                                  model.cost], \n","                                                     {model.inputs: input_batch,\n","                                                      model.targets: target_batch,\n","                                                      model.inputs_length: input_length,\n","                                                      model.targets_length: target_length,\n","                                                      model.keep_prob: 1})\n","\n","                        batch_loss_testing += loss\n","                        end_time_testing = time.time()\n","                        batch_time_testing += end_time_testing - start_time_testing\n","\n","                        test_writer.add_summary(summary, iteration)\n","\n","                    n_batches_testing = batch_i + 1\n","                    print('Testing Loss: {:>6.3f}, Seconds: {:>4.2f}'\n","                          .format(batch_loss_testing / n_batches_testing, \n","                                  batch_time_testing))\n","                    \n","                    batch_time_testing = 0\n","\n","                    testing_loss_summary.append(batch_loss_testing)\n","                    if batch_loss_testing <= min(testing_loss_summary):\n","                        print('New Record!') \n","                        stop_early = 0\n","                        checkpoint = \"./{}.ckpt\".format(log_string)\n","                        saver = tf.train.Saver()\n","                        saver.save(sess, checkpoint)\n","\n","                    else:\n","                        print(\"No Improvement.\")\n","                        stop_early += 1\n","                        if stop_early == stop:\n","                            break\n","\n","            if stop_early == stop:\n","                print(\"Stopping Training.\")\n","                break\n","                "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"VXBq1jNYuWmh","colab_type":"code","outputId":"01727f26-c436-46b2-cd48-14043d6f8e7b","colab":{"base_uri":"https://localhost:8080/","height":53}},"source":["'''\n","* Precondiciones:\n","* Input:\n","* Output:\n","* Postcondiciones:\n","'''\n","for keep_probability in [0.75]:\n","    for num_layers in [2]:\n","        for threshold in [0.95]:\n","            log_string = 'kp={},nl={},th={}'.format(keep_probability,\n","                                                    num_layers,\n","                                                    threshold) \n","            print(\"true\")\n","            model = build_graph(keep_probability, rnn_size, num_layers, batch_size, \n","                                learning_rate, embedding_size, direction)\n","            print(\"true\")\n","            train(model, epochs, log_string)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\n","Training Model: kp=0.75,nl=2,th=0.95\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"rm8AK4rFWu42","colab_type":"text"},"source":["# **Oraciones Erroneas Creadas**"]},{"cell_type":"code","metadata":{"id":"PK2zgS6uuWmn","colab_type":"code","colab":{}},"source":["'''\n","* Precondiciones:\n","* Input:\n","* Output:\n","* Postcondiciones:\n","'''\n","def text_to_ints(text):\n","   \n","    text = clean_text(text)\n","    return [vocab_to_int[word] for word in text]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"X-Fq7dv4uWmq","colab_type":"code","colab":{}},"source":["\n","text = \"Spellin is difficult, whch is wyh you need to study everyday.\"\n","text = text_to_ints(text)\n","\n","\n","checkpoint = \"./kp=0.75,nl=2,th=0.95.ckpt\"\n","\n","model = build_graph(keep_probability, rnn_size, num_layers, batch_size, learning_rate, embedding_size, direction) \n","\n","with tf.Session() as sess:\n","    saver = tf.train.Saver()\n","    saver.restore(sess, checkpoint)\n","    \n","    answer_logits = sess.run(model.predictions, {model.inputs: [text]*batch_size, \n","                                                 model.inputs_length: [len(text)]*batch_size,\n","                                                 model.targets_length: [len(text)+1], \n","                                                 model.keep_prob: [1.0]})[0]\n","\n","pad = vocab_to_int[\"<PAD>\"] \n","\n","print('\\nText')\n","print('  Word Ids:    {}'.format([i for i in text]))\n","print('  Input Words: {}'.format(\"\".join([int_to_vocab[i] for i in text])))\n","\n","print('\\nSummary')\n","print('  Word Ids:       {}'.format([i for i in answer_logits if i != pad]))\n","print('  Response Words: {}'.format(\"\".join([int_to_vocab[i] for i in answer_logits if i != pad])))"],"execution_count":0,"outputs":[]}]}